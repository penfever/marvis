

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>marvis.utils.model_loader &mdash; MARVIS 1.0.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../../_static/custom.css?v=251ceabf" />

  
      <script src="../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../../_static/documentation_options.js?v=8d563738"></script>
      <script src="../../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #2980B9" >

          
          
          <a href="../../../index.html" class="icon icon-home">
            MARVIS
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../getting-started/installation.html">Installation Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../getting-started/quick-start.html">Quick Start Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../getting-started/configuration.html">Configuration Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../user-guide/vision/index.html">Vision Classification Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../user-guide/audio/index.html">Audio Classification Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../user-guide/tabular/index.html">Tabular Data Classification Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../user-guide/api-models/index.html">API Models Integration Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/basic-classification.html">Basic Classification Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/multi-modal-pipeline.html">Multi-Modal Pipeline Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/custom-datasets.html">Custom Datasets Tutorial</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api-reference/marvis.models.html">marvis.models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api-reference/marvis.data.html">marvis.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api-reference/marvis.utils.html">marvis.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api-reference/marvis.viz.html">marvis.viz</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Technical Guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../technical-guides/resource-management.html">Resource Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../technical-guides/caching-system.html">Caching System</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../technical-guides/evaluation-frameworks.html">Evaluation Frameworks</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples &amp; Troubleshooting</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../examples/index.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../troubleshooting.html">Troubleshooting Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../changelog.html">Changelog</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../contributing.html">Contributing to MARVIS</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #2980B9" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">MARVIS</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">marvis.utils.model_loader</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for marvis.utils.model_loader</h1><div class="highlight"><pre>
<span></span><span class="ch">#!/usr/bin/env python</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Centralized model loading system with support for both standard transformers and VLLM backends.</span>

<span class="sd">This module provides a unified interface for loading and using LLMs and VLMs,</span>
<span class="sd">automatically choosing the optimal backend (VLLM for speed when available, </span>
<span class="sd">transformers as fallback) based on model type and availability.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">abc</span><span class="w"> </span><span class="kn">import</span> <span class="n">ABC</span><span class="p">,</span> <span class="n">abstractmethod</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Any</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Union</span><span class="p">,</span> <span class="n">Tuple</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">dataclasses</span><span class="w"> </span><span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>

<span class="c1"># Standard imports</span>
<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoProcessor</span><span class="p">,</span> <span class="n">AutoModelForVision2Seq</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">transformers.models.auto</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>
    <span class="n">TRANSFORMERS_AVAILABLE</span> <span class="o">=</span> <span class="kc">True</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="n">TRANSFORMERS_AVAILABLE</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;transformers not available&quot;</span><span class="p">)</span>

<span class="c1"># VLLM imports</span>
<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">vllm</span><span class="w"> </span><span class="kn">import</span> <span class="n">LLM</span><span class="p">,</span> <span class="n">SamplingParams</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">vllm.model_executor.models.molmo</span><span class="w"> </span><span class="kn">import</span> <span class="n">MolmoForCausalLM</span>
    <span class="n">VLLM_AVAILABLE</span> <span class="o">=</span> <span class="kc">True</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="n">VLLM_AVAILABLE</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;vllm not available, falling back to transformers&quot;</span><span class="p">)</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">PIL</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span>
    <span class="n">PIL_AVAILABLE</span> <span class="o">=</span> <span class="kc">True</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="n">PIL_AVAILABLE</span> <span class="o">=</span> <span class="kc">False</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<div class="viewcode-block" id="GenerationConfig">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.GenerationConfig">[docs]</a>
<span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">GenerationConfig</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Configuration for text generation.&quot;&quot;&quot;</span>
    <span class="n">max_new_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="n">top_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.9</span>
    <span class="n">top_k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span>
    <span class="n">do_sample</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">repetition_penalty</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
    <span class="n">stop_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    
    <span class="c1"># API-specific parameters</span>
    <span class="n">thinking_budget</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># For reasoning models like o3/o4-mini</span>
    <span class="n">enable_thinking</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># For Gemini thinking capabilities</span>
    <span class="n">thinking_summary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>  <span class="c1"># Whether to include thought summaries</span>
    <span class="n">audio_output</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>  <span class="c1"># For models with native audio output</span>
    <span class="n">response_format</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># For structured outputs</span>
    
<div class="viewcode-block" id="GenerationConfig.to_vllm_sampling_params">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.GenerationConfig.to_vllm_sampling_params">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">to_vllm_sampling_params</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s1">&#39;SamplingParams&#39;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Convert to VLLM SamplingParams.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">VLLM_AVAILABLE</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span><span class="s2">&quot;VLLM not available&quot;</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">SamplingParams</span><span class="p">(</span>
            <span class="n">max_tokens</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_new_tokens</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">temperature</span><span class="p">,</span>
            <span class="n">top_p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">top_p</span><span class="p">,</span>
            <span class="n">top_k</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">top_k</span><span class="p">,</span>
            <span class="n">repetition_penalty</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">repetition_penalty</span><span class="p">,</span>
            <span class="n">stop</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stop_tokens</span>
        <span class="p">)</span></div>

    
<div class="viewcode-block" id="GenerationConfig.to_transformers_kwargs">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.GenerationConfig.to_transformers_kwargs">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">to_transformers_kwargs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Convert to transformers generation kwargs.&quot;&quot;&quot;</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;max_new_tokens&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_new_tokens</span><span class="p">,</span>
            <span class="s1">&#39;do_sample&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">do_sample</span><span class="p">,</span>
            <span class="s1">&#39;repetition_penalty&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">repetition_penalty</span>
        <span class="p">}</span>
        
        <span class="c1"># Only add sampling parameters if do_sample is True</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">do_sample</span><span class="p">:</span>
            <span class="n">kwargs</span><span class="o">.</span><span class="n">update</span><span class="p">({</span>
                <span class="s1">&#39;temperature&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">temperature</span><span class="p">,</span>
                <span class="s1">&#39;top_p&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">top_p</span><span class="p">,</span>
                <span class="s1">&#39;top_k&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">top_k</span>
            <span class="p">})</span>
        
        <span class="k">return</span> <span class="n">kwargs</span></div>

    
<div class="viewcode-block" id="GenerationConfig.to_openai_kwargs">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.GenerationConfig.to_openai_kwargs">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">to_openai_kwargs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Convert to OpenAI API kwargs.&quot;&quot;&quot;</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;max_tokens&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_new_tokens</span><span class="p">,</span>
            <span class="s1">&#39;temperature&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">temperature</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">do_sample</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
            <span class="s1">&#39;top_p&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">top_p</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">do_sample</span> <span class="k">else</span> <span class="mi">1</span><span class="p">,</span>
            <span class="s1">&#39;stop&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">stop_tokens</span>
        <span class="p">}</span>
        
        <span class="c1"># Add response format if specified</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">response_format</span><span class="p">:</span>
            <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;response_format&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">response_format</span><span class="p">}</span>
        
        <span class="c1"># Filter out None values</span>
        <span class="k">return</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">}</span></div>

    
<div class="viewcode-block" id="GenerationConfig.to_gemini_kwargs">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.GenerationConfig.to_gemini_kwargs">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">to_gemini_kwargs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Convert to Gemini API kwargs.&quot;&quot;&quot;</span>
        <span class="n">generation_config</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;max_output_tokens&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_new_tokens</span><span class="p">,</span>
            <span class="s1">&#39;temperature&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">temperature</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">do_sample</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
            <span class="s1">&#39;top_p&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">top_p</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">do_sample</span> <span class="k">else</span> <span class="mi">1</span><span class="p">,</span>
            <span class="s1">&#39;top_k&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">top_k</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">do_sample</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s1">&#39;stop_sequences&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">stop_tokens</span>
        <span class="p">}</span>
        
        <span class="c1"># Gemini-specific features</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;generation_config&#39;</span><span class="p">:</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">generation_config</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">}</span>
        <span class="p">}</span>
        
        <span class="c1"># Add thinking controls for Gemini 2.5 models</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;enable_thinking&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">enable_thinking</span><span class="p">:</span>
            <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;thinking&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
            
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;thinking_summary&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">thinking_summary</span><span class="p">:</span>
            <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;include_thinking_summary&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        
        <span class="k">return</span> <span class="n">kwargs</span></div>
</div>



<div class="viewcode-block" id="BaseModelWrapper">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.BaseModelWrapper">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">BaseModelWrapper</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Abstract base class for model wrappers.&quot;&quot;&quot;</span>
    
<div class="viewcode-block" id="BaseModelWrapper.__init__">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.BaseModelWrapper.__init__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span> <span class="o">=</span> <span class="n">model_name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kwargs</span> <span class="o">=</span> <span class="n">kwargs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_model</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span> <span class="o">=</span> <span class="kc">None</span></div>

        
<div class="viewcode-block" id="BaseModelWrapper.load">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.BaseModelWrapper.load">[docs]</a>
    <span class="nd">@abstractmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">load</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Load the model.&quot;&quot;&quot;</span>
        <span class="k">pass</span></div>

    
<div class="viewcode-block" id="BaseModelWrapper.generate">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.BaseModelWrapper.generate">[docs]</a>
    <span class="nd">@abstractmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">generate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span> <span class="n">config</span><span class="p">:</span> <span class="n">GenerationConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate text from inputs.&quot;&quot;&quot;</span>
        <span class="k">pass</span></div>

    
<div class="viewcode-block" id="BaseModelWrapper.is_loaded">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.BaseModelWrapper.is_loaded">[docs]</a>
    <span class="nd">@abstractmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">is_loaded</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Check if model is loaded.&quot;&quot;&quot;</span>
        <span class="k">pass</span></div>

    
<div class="viewcode-block" id="BaseModelWrapper.unload">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.BaseModelWrapper.unload">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">unload</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Unload the model to free memory.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_model&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_model</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_tokenizer&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
        <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="p">,</span> <span class="s1">&#39;mps&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
            <span class="c1"># MPS doesn&#39;t have a direct cache clearing method like CUDA</span>
            <span class="c1"># But we can suggest garbage collection</span>
            <span class="kn">import</span><span class="w"> </span><span class="nn">gc</span>
            <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span></div>

    
<div class="viewcode-block" id="BaseModelWrapper.get_model">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.BaseModelWrapper.get_model">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get the underlying model for compatibility with legacy code.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span></div>

    
<div class="viewcode-block" id="BaseModelWrapper.get_tokenizer">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.BaseModelWrapper.get_tokenizer">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_tokenizer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get the underlying tokenizer for compatibility with legacy code.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_tokenizer&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span></div>
</div>



<div class="viewcode-block" id="VLLMModelWrapper">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.VLLMModelWrapper">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">VLLMModelWrapper</span><span class="p">(</span><span class="n">BaseModelWrapper</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;VLLM-based model wrapper for fast LLM inference.&quot;&quot;&quot;</span>
    
<div class="viewcode-block" id="VLLMModelWrapper.__init__">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.VLLMModelWrapper.__init__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tensor_parallel_size</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;tensor_parallel_size&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gpu_memory_utilization</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;gpu_memory_utilization&#39;</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_model_len</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;max_model_len&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span></div>

        
<div class="viewcode-block" id="VLLMModelWrapper.load">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.VLLMModelWrapper.load">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">load</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Load model using VLLM.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">VLLM_AVAILABLE</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span><span class="s2">&quot;VLLM not available&quot;</span><span class="p">)</span>
            
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="si">}</span><span class="s2"> with VLLM backend&quot;</span><span class="p">)</span>
        
        <span class="c1"># Configure VLLM parameters</span>
        <span class="n">vllm_kwargs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">,</span>
            <span class="s1">&#39;tensor_parallel_size&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensor_parallel_size</span><span class="p">,</span>
            <span class="s1">&#39;gpu_memory_utilization&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">gpu_memory_utilization</span><span class="p">,</span>
            <span class="s1">&#39;trust_remote_code&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
            <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="s1">&#39;auto&#39;</span>
        <span class="p">}</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_model_len</span><span class="p">:</span>
            <span class="n">vllm_kwargs</span><span class="p">[</span><span class="s1">&#39;max_model_len&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_model_len</span>
            
        <span class="c1"># Filter out wrapper-specific kwargs that shouldn&#39;t be passed to VLLM</span>
        <span class="n">wrapper_specific_kwargs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;backend&#39;</span><span class="p">,</span> <span class="s1">&#39;tensor_parallel_size&#39;</span><span class="p">,</span> <span class="s1">&#39;gpu_memory_utilization&#39;</span><span class="p">,</span> 
            <span class="s1">&#39;max_model_len&#39;</span><span class="p">,</span> <span class="s1">&#39;torch_dtype&#39;</span><span class="p">,</span> <span class="s1">&#39;low_cpu_mem_usage&#39;</span><span class="p">,</span> <span class="s1">&#39;device_map&#39;</span><span class="p">,</span> <span class="s1">&#39;use_cache&#39;</span>
        <span class="p">}</span>
        
        <span class="c1"># Add any additional kwargs (excluding wrapper-specific ones)</span>
        <span class="n">vllm_kwargs</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">kwargs</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> 
                           <span class="k">if</span> <span class="n">k</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">wrapper_specific_kwargs</span><span class="p">})</span>
        
        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_model</span> <span class="o">=</span> <span class="n">LLM</span><span class="p">(</span><span class="o">**</span><span class="n">vllm_kwargs</span><span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Successfully loaded </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="si">}</span><span class="s2"> with VLLM&quot;</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Failed to load </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="si">}</span><span class="s2"> with VLLM: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">raise</span></div>

    
<div class="viewcode-block" id="VLLMModelWrapper.generate">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.VLLMModelWrapper.generate">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">generate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span> <span class="n">config</span><span class="p">:</span> <span class="n">GenerationConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate text using VLLM.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_loaded</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Model not loaded&quot;</span><span class="p">)</span>
        
        <span class="c1"># Ensure inputs is a list</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">inputs</span><span class="p">]</span>
            <span class="n">single_input</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">single_input</span> <span class="o">=</span> <span class="kc">False</span>
        
        <span class="c1"># Convert config to VLLM sampling params</span>
        <span class="n">sampling_params</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">to_vllm_sampling_params</span><span class="p">()</span>
        
        <span class="c1"># Generate</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">sampling_params</span><span class="p">)</span>
        
        <span class="c1"># Extract generated text</span>
        <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
            <span class="c1"># Get the generated text (excluding the prompt)</span>
            <span class="n">generated_text</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span>
            <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">generated_text</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">results</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">single_input</span> <span class="k">else</span> <span class="n">results</span></div>

    
<div class="viewcode-block" id="VLLMModelWrapper.is_loaded">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.VLLMModelWrapper.is_loaded">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">is_loaded</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Check if model is loaded.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span></div>
</div>



<div class="viewcode-block" id="VLLMVisionModelWrapper">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.VLLMVisionModelWrapper">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">VLLMVisionModelWrapper</span><span class="p">(</span><span class="n">BaseModelWrapper</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;VLLM-based wrapper for Vision Language Models (multimodal).&quot;&quot;&quot;</span>
    
<div class="viewcode-block" id="VLLMVisionModelWrapper.__init__">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.VLLMVisionModelWrapper.__init__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tensor_parallel_size</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;tensor_parallel_size&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gpu_memory_utilization</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;gpu_memory_utilization&#39;</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_model_len</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;max_model_len&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span> <span class="o">=</span> <span class="kc">None</span></div>

        
<div class="viewcode-block" id="VLLMVisionModelWrapper.load">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.VLLMVisionModelWrapper.load">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">load</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Load VLM using VLLM.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">VLLM_AVAILABLE</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span><span class="s2">&quot;VLLM not available&quot;</span><span class="p">)</span>
            
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="si">}</span><span class="s2"> with VLLM VLM backend&quot;</span><span class="p">)</span>
        
        <span class="c1"># Load tokenizer for chat template support</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">,</span>
                <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span>
            <span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Could not load tokenizer for </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span> <span class="o">=</span> <span class="kc">None</span>
        
        <span class="c1"># Configure VLLM parameters for multimodal models</span>
        <span class="n">vllm_kwargs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">,</span>
            <span class="s1">&#39;tensor_parallel_size&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensor_parallel_size</span><span class="p">,</span>
            <span class="s1">&#39;gpu_memory_utilization&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">gpu_memory_utilization</span><span class="p">,</span>
            <span class="s1">&#39;trust_remote_code&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
            <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="s1">&#39;auto&#39;</span><span class="p">,</span>
            <span class="s1">&#39;max_num_seqs&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>  <span class="c1"># VLMs often work better with single sequence</span>
        <span class="p">}</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_model_len</span><span class="p">:</span>
            <span class="n">vllm_kwargs</span><span class="p">[</span><span class="s1">&#39;max_model_len&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_model_len</span>
            
        <span class="c1"># Filter out wrapper-specific kwargs that shouldn&#39;t be passed to VLLM</span>
        <span class="n">wrapper_specific_kwargs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;backend&#39;</span><span class="p">,</span> <span class="s1">&#39;tensor_parallel_size&#39;</span><span class="p">,</span> <span class="s1">&#39;gpu_memory_utilization&#39;</span><span class="p">,</span> 
            <span class="s1">&#39;max_model_len&#39;</span><span class="p">,</span> <span class="s1">&#39;torch_dtype&#39;</span><span class="p">,</span> <span class="s1">&#39;low_cpu_mem_usage&#39;</span><span class="p">,</span> <span class="s1">&#39;device_map&#39;</span><span class="p">,</span> <span class="s1">&#39;use_cache&#39;</span>
        <span class="p">}</span>
        
        <span class="c1"># Add any additional kwargs (excluding wrapper-specific ones)</span>
        <span class="n">vllm_kwargs</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">kwargs</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> 
                           <span class="k">if</span> <span class="n">k</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">wrapper_specific_kwargs</span><span class="p">})</span>
        
        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_model</span> <span class="o">=</span> <span class="n">LLM</span><span class="p">(</span><span class="o">**</span><span class="n">vllm_kwargs</span><span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Successfully loaded </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="si">}</span><span class="s2"> with VLLM VLM backend&quot;</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Failed to load </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="si">}</span><span class="s2"> with VLLM VLM: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">raise</span></div>

    
<div class="viewcode-block" id="VLLMVisionModelWrapper.generate_from_conversation">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.VLLMVisionModelWrapper.generate_from_conversation">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">generate_from_conversation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">conversation</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">],</span> <span class="n">config</span><span class="p">:</span> <span class="n">GenerationConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate text from a conversation format with image+text input.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_loaded</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Model not loaded&quot;</span><span class="p">)</span>
        
        <span class="c1"># Extract image and text from conversation</span>
        <span class="n">text_content</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
        <span class="n">image</span> <span class="o">=</span> <span class="kc">None</span>
        
        <span class="k">for</span> <span class="n">message</span> <span class="ow">in</span> <span class="n">conversation</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">message</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;content&#39;</span><span class="p">),</span> <span class="nb">list</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">content_item</span> <span class="ow">in</span> <span class="n">message</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]:</span>
                    <span class="k">if</span> <span class="n">content_item</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;type&#39;</span><span class="p">)</span> <span class="o">==</span> <span class="s1">&#39;text&#39;</span><span class="p">:</span>
                        <span class="n">text_content</span> <span class="o">+=</span> <span class="n">content_item</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;text&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span>
                    <span class="k">elif</span> <span class="n">content_item</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;type&#39;</span><span class="p">)</span> <span class="o">==</span> <span class="s1">&#39;image&#39;</span><span class="p">:</span>
                        <span class="n">image</span> <span class="o">=</span> <span class="n">content_item</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;image&#39;</span><span class="p">)</span>
        
        <span class="c1"># For VLLM multimodal, we need to create a proper prompt with image placeholders</span>
        <span class="k">if</span> <span class="n">image</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Try to use tokenizer&#39;s chat template if available</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span><span class="p">,</span> <span class="s1">&#39;apply_chat_template&#39;</span><span class="p">):</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="c1"># Apply chat template</span>
                    <span class="n">formatted_prompt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span>
                        <span class="n">conversation</span><span class="p">,</span>
                        <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span>
                    <span class="p">)</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Applied chat template, prompt: </span><span class="si">{</span><span class="n">formatted_prompt</span><span class="p">[:</span><span class="mi">200</span><span class="p">]</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">)</span>
                <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Failed to apply chat template: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">. Using fallback formatting.&quot;</span><span class="p">)</span>
                    <span class="c1"># Fallback to manual formatting</span>
                    <span class="n">formatted_prompt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_format_prompt_with_placeholder</span><span class="p">(</span><span class="n">text_content</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Manual formatting with image placeholders</span>
                <span class="n">formatted_prompt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_format_prompt_with_placeholder</span><span class="p">(</span><span class="n">text_content</span><span class="p">)</span>
            
            <span class="c1"># Create the prompt dictionary for VLLM</span>
            <span class="n">prompt</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="n">formatted_prompt</span><span class="p">,</span>
                <span class="s2">&quot;multi_modal_data&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;image&quot;</span><span class="p">:</span> <span class="n">image</span><span class="p">}</span>
            <span class="p">}</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Text-only prompt</span>
            <span class="n">prompt</span> <span class="o">=</span> <span class="n">text_content</span>
        
        <span class="c1"># Convert config to VLLM sampling params</span>
        <span class="n">sampling_params</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">to_vllm_sampling_params</span><span class="p">()</span>
        
        <span class="c1"># Generate</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">sampling_params</span><span class="p">)</span>
        
        <span class="c1"># Extract generated text</span>
        <span class="n">generated_text</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span>
        
        <span class="k">return</span> <span class="n">generated_text</span></div>

    
    <span class="k">def</span><span class="w"> </span><span class="nf">_format_prompt_with_placeholder</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text_content</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Format prompt with appropriate image placeholder for the model.&quot;&quot;&quot;</span>
        <span class="c1"># Determine the correct image placeholder based on model type</span>
        <span class="n">image_placeholder</span> <span class="o">=</span> <span class="s2">&quot;&lt;image&gt;&quot;</span>  <span class="c1"># Default placeholder</span>
        
        <span class="c1"># Model-specific placeholders</span>
        <span class="k">if</span> <span class="s2">&quot;Qwen&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">:</span>
            <span class="c1"># Qwen models use &lt;|image_pad|&gt; or &lt;|vision_start|&gt;&lt;|image_pad|&gt;&lt;|vision_end|&gt;</span>
            <span class="n">image_placeholder</span> <span class="o">=</span> <span class="s2">&quot;&lt;|vision_start|&gt;&lt;|image_pad|&gt;&lt;|vision_end|&gt;&quot;</span>
        <span class="k">elif</span> <span class="s2">&quot;llava&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
            <span class="n">image_placeholder</span> <span class="o">=</span> <span class="s2">&quot;&lt;image&gt;&quot;</span>
        <span class="k">elif</span> <span class="s2">&quot;Phi-3.5-vision&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">:</span>
            <span class="n">image_placeholder</span> <span class="o">=</span> <span class="s2">&quot;&lt;|image_1|&gt;&quot;</span>
        <span class="k">elif</span> <span class="s2">&quot;pixtral&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
            <span class="n">image_placeholder</span> <span class="o">=</span> <span class="s2">&quot;[IMG]&quot;</span>
        <span class="k">elif</span> <span class="s2">&quot;molmo&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
            <span class="n">image_placeholder</span> <span class="o">=</span> <span class="s2">&quot;&lt;image&gt;&quot;</span>
        <span class="k">elif</span> <span class="s2">&quot;Llama-3.2&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span> <span class="ow">and</span> <span class="s2">&quot;Vision&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">:</span>
            <span class="n">image_placeholder</span> <span class="o">=</span> <span class="s2">&quot;&lt;|image|&gt;&quot;</span>
        
        <span class="c1"># Insert image placeholder at the beginning of the prompt</span>
        <span class="n">formatted_prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">image_placeholder</span><span class="si">}</span><span class="se">\n</span><span class="si">{</span><span class="n">text_content</span><span class="si">}</span><span class="s2">&quot;</span>
        
        <span class="k">return</span> <span class="n">formatted_prompt</span>
    
<div class="viewcode-block" id="VLLMVisionModelWrapper.generate">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.VLLMVisionModelWrapper.generate">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">generate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span> <span class="n">config</span><span class="p">:</span> <span class="n">GenerationConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Standard generate method for text-only inputs.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">inputs</span><span class="p">]</span>
            <span class="n">single_input</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">single_input</span> <span class="o">=</span> <span class="kc">False</span>
        
        <span class="c1"># Convert to VLLM format and generate</span>
        <span class="n">sampling_params</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">to_vllm_sampling_params</span><span class="p">()</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">sampling_params</span><span class="p">)</span>
        
        <span class="c1"># Extract generated text</span>
        <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
            <span class="n">generated_text</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span>
            <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">generated_text</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">results</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">single_input</span> <span class="k">else</span> <span class="n">results</span></div>

    
<div class="viewcode-block" id="VLLMVisionModelWrapper.is_loaded">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.VLLMVisionModelWrapper.is_loaded">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">is_loaded</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Check if model is loaded.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span></div>
</div>



<div class="viewcode-block" id="TransformersModelWrapper">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.TransformersModelWrapper">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">TransformersModelWrapper</span><span class="p">(</span><span class="n">BaseModelWrapper</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Transformers-based model wrapper.&quot;&quot;&quot;</span>
    
<div class="viewcode-block" id="TransformersModelWrapper.__init__">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.TransformersModelWrapper.__init__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">torch_dtype</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;torch_dtype&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">low_cpu_mem_usage</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;low_cpu_mem_usage&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device_map</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;device_map&#39;</span><span class="p">,</span> <span class="s1">&#39;auto&#39;</span> <span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="s1">&#39;auto&#39;</span> <span class="k">else</span> <span class="kc">None</span><span class="p">)</span></div>

        
<div class="viewcode-block" id="TransformersModelWrapper.load">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.TransformersModelWrapper.load">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">load</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Load model using transformers.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">TRANSFORMERS_AVAILABLE</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span><span class="s2">&quot;Transformers not available&quot;</span><span class="p">)</span>
            
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="si">}</span><span class="s2"> with transformers backend&quot;</span><span class="p">)</span>
        
        <span class="c1"># Load tokenizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">,</span>
            <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        
        <span class="c1"># Add pad token if missing</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
        
        <span class="c1"># Configure model loading parameters</span>
        <span class="n">model_kwargs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;trust_remote_code&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
            <span class="s1">&#39;low_cpu_mem_usage&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">low_cpu_mem_usage</span>
        <span class="p">}</span>
        
        <span class="c1"># Device and dtype configuration</span>
        <span class="c1"># Import device utilities for proper device detection</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">marvis.utils.device_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">detect_optimal_device</span>
        
        <span class="c1"># Resolve device if set to auto</span>
        <span class="n">actual_device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="s2">&quot;auto&quot;</span><span class="p">:</span>
            <span class="n">actual_device</span> <span class="o">=</span> <span class="n">detect_optimal_device</span><span class="p">(</span><span class="n">prefer_mps</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Auto-detected device: </span><span class="si">{</span><span class="n">actual_device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        
        <span class="c1"># Configure based on device</span>
        <span class="k">if</span> <span class="n">actual_device</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
            <span class="n">model_kwargs</span><span class="o">.</span><span class="n">update</span><span class="p">({</span>
                <span class="s1">&#39;torch_dtype&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">torch_dtype</span><span class="p">,</span>
                <span class="s1">&#39;device_map&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">device_map</span>
            <span class="p">})</span>
        <span class="k">elif</span> <span class="n">actual_device</span> <span class="o">==</span> <span class="s2">&quot;mps&quot;</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="p">,</span> <span class="s1">&#39;mps&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
            <span class="c1"># MPS configuration - don&#39;t use device_map, load to CPU then move to MPS</span>
            <span class="n">model_kwargs</span><span class="o">.</span><span class="n">update</span><span class="p">({</span>
                <span class="s1">&#39;torch_dtype&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>  <span class="c1"># MPS works better with float32</span>
                <span class="s1">&#39;device_map&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># Don&#39;t use device_map with MPS</span>
                <span class="s1">&#39;low_cpu_mem_usage&#39;</span><span class="p">:</span> <span class="kc">True</span>  <span class="c1"># Use CPU-efficient loading</span>
            <span class="p">})</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Using MPS (Metal Performance Shaders) for GPU acceleration&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">actual_device</span> <span class="o">==</span> <span class="s2">&quot;cpu&quot;</span><span class="p">:</span>
            <span class="n">model_kwargs</span><span class="o">.</span><span class="n">update</span><span class="p">({</span>
                <span class="s1">&#39;torch_dtype&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>  <span class="c1"># Use float32 for CPU</span>
                <span class="s1">&#39;device_map&#39;</span><span class="p">:</span> <span class="s1">&#39;cpu&#39;</span>
            <span class="p">})</span>
        
        <span class="c1"># Filter out wrapper-specific kwargs that shouldn&#39;t be passed to the model</span>
        <span class="n">wrapper_specific_kwargs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;backend&#39;</span><span class="p">,</span> <span class="s1">&#39;tensor_parallel_size&#39;</span><span class="p">,</span> <span class="s1">&#39;gpu_memory_utilization&#39;</span><span class="p">,</span> 
            <span class="s1">&#39;torch_dtype&#39;</span><span class="p">,</span> <span class="s1">&#39;low_cpu_mem_usage&#39;</span><span class="p">,</span> <span class="s1">&#39;device_map&#39;</span>
        <span class="p">}</span>
        
        <span class="c1"># Add any additional kwargs (excluding wrapper-specific ones)</span>
        <span class="n">model_kwargs</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">kwargs</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> 
                           <span class="k">if</span> <span class="n">k</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">wrapper_specific_kwargs</span><span class="p">})</span>
        
        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">,</span>
                <span class="o">**</span><span class="n">model_kwargs</span>
            <span class="p">)</span>
            
            <span class="c1"># Manually move to MPS if needed (since we loaded to CPU first)</span>
            <span class="k">if</span> <span class="n">actual_device</span> <span class="o">==</span> <span class="s2">&quot;mps&quot;</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="p">,</span> <span class="s1">&#39;mps&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Moving model to MPS device...&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;mps&#39;</span><span class="p">))</span>
                
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Successfully loaded </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="si">}</span><span class="s2"> with transformers&quot;</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Failed to load </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="si">}</span><span class="s2"> with transformers: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">raise</span></div>

    
<div class="viewcode-block" id="TransformersModelWrapper.generate">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.TransformersModelWrapper.generate">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">generate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span> <span class="n">config</span><span class="p">:</span> <span class="n">GenerationConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate text using transformers.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_loaded</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Model not loaded&quot;</span><span class="p">)</span>
        
        <span class="c1"># Ensure inputs is a list</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">inputs</span><span class="p">]</span>
            <span class="n">single_input</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">single_input</span> <span class="o">=</span> <span class="kc">False</span>
        
        <span class="c1"># Tokenize inputs</span>
        <span class="n">tokenized</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        
        <span class="c1"># Move to device  </span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="p">,</span> <span class="s1">&#39;device&#39;</span><span class="p">):</span>
            <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">device</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Try to infer device from model parameters</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">device</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span>
            <span class="k">except</span><span class="p">:</span>
                <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
        
        <span class="c1"># Move inputs to the appropriate device</span>
        <span class="k">if</span> <span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">!=</span> <span class="s1">&#39;cpu&#39;</span><span class="p">:</span>
            <span class="n">tokenized</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">tokenized</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
        
        <span class="c1"># Generate</span>
        <span class="n">gen_kwargs</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">to_transformers_kwargs</span><span class="p">()</span>
        <span class="n">gen_kwargs</span><span class="p">[</span><span class="s1">&#39;pad_token_id&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span>
        
        <span class="c1"># Some models (like Qwen2.5) don&#39;t support certain generation flags</span>
        <span class="c1"># Use a more conservative approach to avoid warnings</span>
        <span class="n">safe_kwargs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;max_new_tokens&#39;</span><span class="p">:</span> <span class="n">gen_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;max_new_tokens&#39;</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span>
            <span class="s1">&#39;do_sample&#39;</span><span class="p">:</span> <span class="n">gen_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;do_sample&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
            <span class="s1">&#39;pad_token_id&#39;</span><span class="p">:</span> <span class="n">gen_kwargs</span><span class="p">[</span><span class="s1">&#39;pad_token_id&#39;</span><span class="p">]</span>
        <span class="p">}</span>
        
        <span class="c1"># Only add sampling parameters if do_sample is True and model supports them</span>
        <span class="k">if</span> <span class="n">gen_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;do_sample&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
            <span class="c1"># Use a conservative temperature to avoid warnings</span>
            <span class="n">safe_kwargs</span><span class="p">[</span><span class="s1">&#39;temperature&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">gen_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;temperature&#39;</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">))</span>
            <span class="c1"># Skip top_p and top_k for models that don&#39;t support them reliably</span>
            <span class="k">if</span> <span class="s1">&#39;qwen&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
                <span class="n">safe_kwargs</span><span class="p">[</span><span class="s1">&#39;top_p&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gen_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;top_p&#39;</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">)</span>
                <span class="n">safe_kwargs</span><span class="p">[</span><span class="s1">&#39;top_k&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gen_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;top_k&#39;</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
        
        <span class="c1"># Add repetition penalty if present</span>
        <span class="k">if</span> <span class="s1">&#39;repetition_penalty&#39;</span> <span class="ow">in</span> <span class="n">gen_kwargs</span><span class="p">:</span>
            <span class="n">safe_kwargs</span><span class="p">[</span><span class="s1">&#39;repetition_penalty&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gen_kwargs</span><span class="p">[</span><span class="s1">&#39;repetition_penalty&#39;</span><span class="p">]</span>
        
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
                <span class="o">**</span><span class="n">tokenized</span><span class="p">,</span>
                <span class="o">**</span><span class="n">safe_kwargs</span>
            <span class="p">)</span>
        
        <span class="c1"># Decode outputs</span>
        <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">output</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">outputs</span><span class="p">):</span>
            <span class="c1"># Remove input tokens to get only generated text</span>
            <span class="n">input_length</span> <span class="o">=</span> <span class="n">tokenized</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">generated_tokens</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="n">input_length</span><span class="p">:]</span>
            <span class="n">generated_text</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">generated_tokens</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">generated_text</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">results</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">single_input</span> <span class="k">else</span> <span class="n">results</span></div>

    
<div class="viewcode-block" id="TransformersModelWrapper.is_loaded">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.TransformersModelWrapper.is_loaded">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">is_loaded</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Check if model is loaded.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span></div>
</div>



<div class="viewcode-block" id="VisionLanguageModelWrapper">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.VisionLanguageModelWrapper">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">VisionLanguageModelWrapper</span><span class="p">(</span><span class="n">BaseModelWrapper</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Wrapper for Vision Language Models using transformers.&quot;&quot;&quot;</span>
    
<div class="viewcode-block" id="VisionLanguageModelWrapper.__init__">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.VisionLanguageModelWrapper.__init__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">torch_dtype</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;torch_dtype&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">low_cpu_mem_usage</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;low_cpu_mem_usage&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device_map</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;device_map&#39;</span><span class="p">,</span> <span class="s1">&#39;auto&#39;</span> <span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="s1">&#39;auto&#39;</span> <span class="k">else</span> <span class="kc">None</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_processor</span> <span class="o">=</span> <span class="kc">None</span></div>

        
<div class="viewcode-block" id="VisionLanguageModelWrapper.load">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.VisionLanguageModelWrapper.load">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">load</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Load VLM using transformers.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">TRANSFORMERS_AVAILABLE</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span><span class="s2">&quot;Transformers not available&quot;</span><span class="p">)</span>
            
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="si">}</span><span class="s2"> with transformers VLM backend&quot;</span><span class="p">)</span>
        
        <span class="c1"># Load processor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_processor</span> <span class="o">=</span> <span class="n">AutoProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">,</span>
            <span class="n">use_fast</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        
        <span class="c1"># Configure model loading parameters</span>
        <span class="n">model_kwargs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;trust_remote_code&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
            <span class="s1">&#39;low_cpu_mem_usage&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">low_cpu_mem_usage</span>
        <span class="p">}</span>
        
        <span class="c1"># Device and dtype configuration</span>
        <span class="c1"># Import device utilities for proper device detection</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">marvis.utils.device_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">detect_optimal_device</span>
        
        <span class="c1"># Resolve device if set to auto</span>
        <span class="n">actual_device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="s2">&quot;auto&quot;</span><span class="p">:</span>
            <span class="n">actual_device</span> <span class="o">=</span> <span class="n">detect_optimal_device</span><span class="p">(</span><span class="n">prefer_mps</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Auto-detected device: </span><span class="si">{</span><span class="n">actual_device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        
        <span class="c1"># Configure based on device</span>
        <span class="k">if</span> <span class="n">actual_device</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
            <span class="n">model_kwargs</span><span class="o">.</span><span class="n">update</span><span class="p">({</span>
                <span class="s1">&#39;torch_dtype&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">torch_dtype</span><span class="p">,</span>
                <span class="s1">&#39;device_map&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">device_map</span>
            <span class="p">})</span>
        <span class="k">elif</span> <span class="n">actual_device</span> <span class="o">==</span> <span class="s2">&quot;mps&quot;</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="p">,</span> <span class="s1">&#39;mps&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
            <span class="c1"># MPS configuration - don&#39;t use device_map, load to CPU then move to MPS</span>
            <span class="n">model_kwargs</span><span class="o">.</span><span class="n">update</span><span class="p">({</span>
                <span class="s1">&#39;torch_dtype&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                <span class="s1">&#39;device_map&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># Don&#39;t use device_map with MPS</span>
                <span class="s1">&#39;low_cpu_mem_usage&#39;</span><span class="p">:</span> <span class="kc">True</span>  <span class="c1"># Use CPU-efficient loading</span>
            <span class="p">})</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Using MPS (Metal Performance Shaders) for GPU acceleration&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">actual_device</span> <span class="o">==</span> <span class="s2">&quot;cpu&quot;</span><span class="p">:</span>
            <span class="n">model_kwargs</span><span class="o">.</span><span class="n">update</span><span class="p">({</span>
                <span class="s1">&#39;torch_dtype&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                <span class="s1">&#39;device_map&#39;</span><span class="p">:</span> <span class="s1">&#39;cpu&#39;</span>
            <span class="p">})</span>
        
        <span class="c1"># Filter out wrapper-specific kwargs that shouldn&#39;t be passed to the model</span>
        <span class="n">wrapper_specific_kwargs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;backend&#39;</span><span class="p">,</span> <span class="s1">&#39;tensor_parallel_size&#39;</span><span class="p">,</span> <span class="s1">&#39;gpu_memory_utilization&#39;</span><span class="p">,</span> 
            <span class="s1">&#39;max_model_len&#39;</span><span class="p">,</span> <span class="s1">&#39;torch_dtype&#39;</span><span class="p">,</span> <span class="s1">&#39;low_cpu_mem_usage&#39;</span><span class="p">,</span> <span class="s1">&#39;device_map&#39;</span>
        <span class="p">}</span>
        
        <span class="c1"># Add any additional kwargs (excluding wrapper-specific ones)</span>
        <span class="n">model_kwargs</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">kwargs</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> 
                           <span class="k">if</span> <span class="n">k</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">wrapper_specific_kwargs</span><span class="p">})</span>
        
        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_model</span> <span class="o">=</span> <span class="n">AutoModelForVision2Seq</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">,</span>
                <span class="o">**</span><span class="n">model_kwargs</span>
            <span class="p">)</span>
            
            <span class="c1"># Manually move to MPS if needed (since we loaded to CPU first)</span>
            <span class="k">if</span> <span class="n">actual_device</span> <span class="o">==</span> <span class="s2">&quot;mps&quot;</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="p">,</span> <span class="s1">&#39;mps&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Moving model to MPS device...&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;mps&#39;</span><span class="p">))</span>
                
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Successfully loaded </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="si">}</span><span class="s2"> with transformers VLM&quot;</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Failed to load </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="si">}</span><span class="s2"> with transformers VLM: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">raise</span></div>

    
<div class="viewcode-block" id="VisionLanguageModelWrapper.generate_from_conversation">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.VisionLanguageModelWrapper.generate_from_conversation">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">generate_from_conversation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">conversation</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">],</span> <span class="n">config</span><span class="p">:</span> <span class="n">GenerationConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate text from a conversation format (for VLMs with image+text input).&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_loaded</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Model not loaded&quot;</span><span class="p">)</span>
        
        <span class="c1"># Process the conversation</span>
        <span class="n">formatted_text</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_processor</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span>
            <span class="n">conversation</span><span class="p">,</span> 
            <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
            <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        
        <span class="c1"># Extract image from conversation</span>
        <span class="n">image</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">for</span> <span class="n">message</span> <span class="ow">in</span> <span class="n">conversation</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">message</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;content&#39;</span><span class="p">),</span> <span class="nb">list</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">content_item</span> <span class="ow">in</span> <span class="n">message</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]:</span>
                    <span class="k">if</span> <span class="n">content_item</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;type&#39;</span><span class="p">)</span> <span class="o">==</span> <span class="s1">&#39;image&#39;</span><span class="p">:</span>
                        <span class="n">image</span> <span class="o">=</span> <span class="n">content_item</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;image&#39;</span><span class="p">)</span>
                        <span class="k">break</span>
                <span class="k">if</span> <span class="n">image</span><span class="p">:</span>
                    <span class="k">break</span>
        
        <span class="c1"># Process inputs</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_processor</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">formatted_text</span><span class="p">,</span> <span class="n">images</span><span class="o">=</span><span class="n">image</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
        
        <span class="c1"># Move to device</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="p">,</span> <span class="s1">&#39;device&#39;</span><span class="p">):</span>
            <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">device</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Try to infer device from model parameters</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">device</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span>
            <span class="k">except</span><span class="p">:</span>
                <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
        
        <span class="c1"># Move inputs to the appropriate device</span>
        <span class="k">if</span> <span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">!=</span> <span class="s1">&#39;cpu&#39;</span><span class="p">:</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">else</span> <span class="n">v</span> 
                     <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">inputs</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
        
        <span class="c1"># Generate</span>
        <span class="n">gen_kwargs</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">to_transformers_kwargs</span><span class="p">()</span>
        <span class="n">gen_kwargs</span><span class="p">[</span><span class="s1">&#39;pad_token_id&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_processor</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span>
        
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">generate_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
                <span class="o">**</span><span class="n">inputs</span><span class="p">,</span>
                <span class="o">**</span><span class="n">gen_kwargs</span>
            <span class="p">)</span>
        
        <span class="c1"># Decode response</span>
        <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_processor</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span>
            <span class="n">generate_ids</span><span class="p">[:,</span> <span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:],</span> 
            <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">clean_up_tokenization_spaces</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="k">return</span> <span class="n">response</span></div>

    
<div class="viewcode-block" id="VisionLanguageModelWrapper.generate">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.VisionLanguageModelWrapper.generate">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">generate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span> <span class="n">config</span><span class="p">:</span> <span class="n">GenerationConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Standard generate method for text-only inputs.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">inputs</span><span class="p">]</span>
            <span class="n">single_input</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">single_input</span> <span class="o">=</span> <span class="kc">False</span>
        
        <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">text_input</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
            <span class="c1"># Create a simple conversation for text-only input</span>
            <span class="n">conversation</span> <span class="o">=</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;text&quot;</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="n">text_input</span><span class="p">}]}]</span>
            <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generate_from_conversation</span><span class="p">(</span><span class="n">conversation</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>
            <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">results</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">single_input</span> <span class="k">else</span> <span class="n">results</span></div>

    
<div class="viewcode-block" id="VisionLanguageModelWrapper.is_loaded">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.VisionLanguageModelWrapper.is_loaded">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">is_loaded</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Check if model is loaded.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_processor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span></div>

    
<div class="viewcode-block" id="VisionLanguageModelWrapper.unload">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.VisionLanguageModelWrapper.unload">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">unload</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Unload the model to free memory.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">unload</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_processor&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_processor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_processor</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_processor</span> <span class="o">=</span> <span class="kc">None</span></div>
</div>



<div class="viewcode-block" id="OpenAIModelWrapper">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.OpenAIModelWrapper">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">OpenAIModelWrapper</span><span class="p">(</span><span class="n">BaseModelWrapper</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;OpenAI API-based model wrapper for LLM tasks.&quot;&quot;&quot;</span>
    
<div class="viewcode-block" id="OpenAIModelWrapper.__init__">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.OpenAIModelWrapper.__init__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_client</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># Handle max_model_len for API models - store for use in generation config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_model_len</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;max_model_len&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span></div>

        
<div class="viewcode-block" id="OpenAIModelWrapper.load">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.OpenAIModelWrapper.load">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">load</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Load OpenAI API client.&quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="kn">import</span><span class="w"> </span><span class="nn">openai</span>
            <span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
            
            <span class="n">api_key</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;OPENAI_API_KEY&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">api_key</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;OPENAI_API_KEY environment variable not set&quot;</span><span class="p">)</span>
            
            <span class="bp">self</span><span class="o">.</span><span class="n">_client</span> <span class="o">=</span> <span class="n">openai</span><span class="o">.</span><span class="n">OpenAI</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Successfully connected to OpenAI API for model: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            
        <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span><span class="s2">&quot;OpenAI package not available. Install with: pip install &#39;marvis[api]&#39;&quot;</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Failed to initialize OpenAI client: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">raise</span></div>

    
<div class="viewcode-block" id="OpenAIModelWrapper.generate">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.OpenAIModelWrapper.generate">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">generate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span> <span class="n">config</span><span class="p">:</span> <span class="n">GenerationConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate text using OpenAI API.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_loaded</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Model not loaded&quot;</span><span class="p">)</span>
        
        <span class="c1"># Ensure inputs is a list</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">inputs</span><span class="p">]</span>
            <span class="n">single_input</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">single_input</span> <span class="o">=</span> <span class="kc">False</span>
        
        <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># Override max_new_tokens with max_model_len if provided</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_model_len</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">config</span> <span class="o">=</span> <span class="n">GenerationConfig</span><span class="p">(</span>
                <span class="n">max_new_tokens</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_model_len</span><span class="p">,</span>
                <span class="n">temperature</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">temperature</span><span class="p">,</span>
                <span class="n">top_p</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">top_p</span><span class="p">,</span>
                <span class="n">top_k</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">top_k</span><span class="p">,</span>
                <span class="n">do_sample</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">do_sample</span><span class="p">,</span>
                <span class="n">repetition_penalty</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">repetition_penalty</span><span class="p">,</span>
                <span class="n">stop_tokens</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">stop_tokens</span>
            <span class="p">)</span>
        <span class="n">generation_kwargs</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">to_openai_kwargs</span><span class="p">()</span>
        
        <span class="k">for</span> <span class="n">text_input</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
            <span class="c1"># Retry delays: 15s, 15s, 30s, 30s, 60s</span>
            <span class="n">retry_delays</span> <span class="o">=</span> <span class="p">[</span><span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">60</span><span class="p">]</span>
            <span class="n">max_retries</span> <span class="o">=</span> <span class="mi">5</span>
            
            <span class="k">for</span> <span class="n">attempt</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_retries</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="c1"># Use chat completions for all models</span>
                    <span class="n">messages</span> <span class="o">=</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">text_input</span><span class="p">}]</span>
                    
                    <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
                        <span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">,</span>
                        <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                        <span class="o">**</span><span class="n">generation_kwargs</span>
                    <span class="p">)</span>
                    
                    <span class="n">generated_text</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span>
                    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">generated_text</span><span class="p">)</span>
                    <span class="k">break</span>  <span class="c1"># Success, exit retry loop</span>
                    
                <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                    <span class="c1"># Check if it&#39;s a rate limit error (429)</span>
                    <span class="n">is_rate_limit</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="nb">hasattr</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="s1">&#39;status_code&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">e</span><span class="o">.</span><span class="n">status_code</span> <span class="o">==</span> <span class="mi">429</span>
                    <span class="p">)</span> <span class="ow">or</span> <span class="s1">&#39;429&#39;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span> <span class="ow">or</span> <span class="s1">&#39;rate limit&#39;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
                    
                    <span class="k">if</span> <span class="n">is_rate_limit</span> <span class="ow">and</span> <span class="n">attempt</span> <span class="o">&lt;</span> <span class="n">max_retries</span><span class="p">:</span>
                        <span class="n">delay</span> <span class="o">=</span> <span class="n">retry_delays</span><span class="p">[</span><span class="n">attempt</span><span class="p">]</span>
                        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Rate limit error (attempt </span><span class="si">{</span><span class="n">attempt</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">max_retries</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">). Retrying in </span><span class="si">{</span><span class="n">delay</span><span class="si">}</span><span class="s2">s...&quot;</span><span class="p">)</span>
                        <span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
                        <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="n">delay</span><span class="p">)</span>
                        <span class="k">continue</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="c1"># Either not a rate limit error, or we&#39;ve exhausted retries</span>
                        <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;OpenAI API error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;OpenAI API call failed: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span> <span class="kn">from</span><span class="w"> </span><span class="nn">e</span>
        
        <span class="k">return</span> <span class="n">results</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">single_input</span> <span class="k">else</span> <span class="n">results</span></div>

    
<div class="viewcode-block" id="OpenAIModelWrapper.is_loaded">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.OpenAIModelWrapper.is_loaded">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">is_loaded</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Check if API client is initialized.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_client</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span></div>

    
<div class="viewcode-block" id="OpenAIModelWrapper.unload">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.OpenAIModelWrapper.unload">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">unload</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Cleanup API client.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_client</span> <span class="o">=</span> <span class="kc">None</span></div>
</div>



<div class="viewcode-block" id="OpenAIVisionModelWrapper">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.OpenAIVisionModelWrapper">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">OpenAIVisionModelWrapper</span><span class="p">(</span><span class="n">BaseModelWrapper</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;OpenAI API-based wrapper for Vision Language Models.&quot;&quot;&quot;</span>
    
<div class="viewcode-block" id="OpenAIVisionModelWrapper.__init__">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.OpenAIVisionModelWrapper.__init__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_client</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># Handle max_model_len for API models - store for use in generation config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_model_len</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;max_model_len&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span></div>

        
<div class="viewcode-block" id="OpenAIVisionModelWrapper.load">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.OpenAIVisionModelWrapper.load">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">load</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Load OpenAI API client.&quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="kn">import</span><span class="w"> </span><span class="nn">openai</span>
            <span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
            
            <span class="n">api_key</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;OPENAI_API_KEY&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">api_key</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;OPENAI_API_KEY environment variable not set&quot;</span><span class="p">)</span>
            
            <span class="bp">self</span><span class="o">.</span><span class="n">_client</span> <span class="o">=</span> <span class="n">openai</span><span class="o">.</span><span class="n">OpenAI</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Successfully connected to OpenAI API for VLM: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            
        <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span><span class="s2">&quot;OpenAI package not available. Install with: pip install &#39;marvis[api]&#39;&quot;</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Failed to initialize OpenAI client: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">raise</span></div>

    
<div class="viewcode-block" id="OpenAIVisionModelWrapper.generate_from_conversation">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.OpenAIVisionModelWrapper.generate_from_conversation">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">generate_from_conversation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">conversation</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">],</span> <span class="n">config</span><span class="p">:</span> <span class="n">GenerationConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate text from a conversation format with image+text input.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_loaded</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Model not loaded&quot;</span><span class="p">)</span>
        
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Convert conversation format to OpenAI format</span>
            <span class="n">messages</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">message</span> <span class="ow">in</span> <span class="n">conversation</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">message</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;content&#39;</span><span class="p">),</span> <span class="nb">list</span><span class="p">):</span>
                    <span class="c1"># Handle multimodal content</span>
                    <span class="n">openai_content</span> <span class="o">=</span> <span class="p">[]</span>
                    <span class="k">for</span> <span class="n">content_item</span> <span class="ow">in</span> <span class="n">message</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]:</span>
                        <span class="k">if</span> <span class="n">content_item</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;type&#39;</span><span class="p">)</span> <span class="o">==</span> <span class="s1">&#39;text&#39;</span><span class="p">:</span>
                            <span class="n">openai_content</span><span class="o">.</span><span class="n">append</span><span class="p">({</span>
                                <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;text&quot;</span><span class="p">,</span>
                                <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="n">content_item</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;text&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span>
                            <span class="p">})</span>
                        <span class="k">elif</span> <span class="n">content_item</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;type&#39;</span><span class="p">)</span> <span class="o">==</span> <span class="s1">&#39;image&#39;</span><span class="p">:</span>
                            <span class="c1"># Handle PIL Image objects</span>
                            <span class="n">image</span> <span class="o">=</span> <span class="n">content_item</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;image&#39;</span><span class="p">)</span>
                            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="s1">&#39;save&#39;</span><span class="p">):</span>  <span class="c1"># PIL Image</span>
                                <span class="kn">import</span><span class="w"> </span><span class="nn">base64</span>
                                <span class="kn">import</span><span class="w"> </span><span class="nn">io</span>
                                
                                <span class="c1"># Convert PIL Image to base64</span>
                                <span class="n">buffer</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">BytesIO</span><span class="p">()</span>
                                <span class="n">image</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s1">&#39;PNG&#39;</span><span class="p">)</span>
                                <span class="n">image_data</span> <span class="o">=</span> <span class="n">base64</span><span class="o">.</span><span class="n">b64encode</span><span class="p">(</span><span class="n">buffer</span><span class="o">.</span><span class="n">getvalue</span><span class="p">())</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span>
                                
                                <span class="n">openai_content</span><span class="o">.</span><span class="n">append</span><span class="p">({</span>
                                    <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;image_url&quot;</span><span class="p">,</span>
                                    <span class="s2">&quot;image_url&quot;</span><span class="p">:</span> <span class="p">{</span>
                                        <span class="s2">&quot;url&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;data:image/png;base64,</span><span class="si">{</span><span class="n">image_data</span><span class="si">}</span><span class="s2">&quot;</span>
                                    <span class="p">}</span>
                                <span class="p">})</span>
                    
                    <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">({</span>
                        <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="n">message</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;role&#39;</span><span class="p">,</span> <span class="s1">&#39;user&#39;</span><span class="p">),</span>
                        <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">openai_content</span>
                    <span class="p">})</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Text-only content</span>
                    <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">({</span>
                        <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="n">message</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;role&#39;</span><span class="p">,</span> <span class="s1">&#39;user&#39;</span><span class="p">),</span>
                        <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">message</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;content&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span>
                    <span class="p">})</span>
            
            <span class="c1"># Override max_new_tokens with max_model_len if provided</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_model_len</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">config</span> <span class="o">=</span> <span class="n">GenerationConfig</span><span class="p">(</span>
                    <span class="n">max_new_tokens</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_model_len</span><span class="p">,</span>
                    <span class="n">temperature</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">temperature</span><span class="p">,</span>
                    <span class="n">top_p</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">top_p</span><span class="p">,</span>
                    <span class="n">top_k</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">top_k</span><span class="p">,</span>
                    <span class="n">do_sample</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">do_sample</span><span class="p">,</span>
                    <span class="n">repetition_penalty</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">repetition_penalty</span><span class="p">,</span>
                    <span class="n">stop_tokens</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">stop_tokens</span>
                <span class="p">)</span>
            <span class="n">generation_kwargs</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">to_openai_kwargs</span><span class="p">()</span>
            
            <span class="c1"># Retry logic for rate limits</span>
            <span class="n">retry_delays</span> <span class="o">=</span> <span class="p">[</span><span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">60</span><span class="p">]</span>
            <span class="n">max_retries</span> <span class="o">=</span> <span class="mi">5</span>
            
            <span class="k">for</span> <span class="n">attempt</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_retries</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
                        <span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">,</span>
                        <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                        <span class="o">**</span><span class="n">generation_kwargs</span>
                    <span class="p">)</span>
                    
                    <span class="k">return</span> <span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span>
                    
                <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                    <span class="c1"># Check if it&#39;s a rate limit error (429)</span>
                    <span class="n">is_rate_limit</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="nb">hasattr</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="s1">&#39;status_code&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">e</span><span class="o">.</span><span class="n">status_code</span> <span class="o">==</span> <span class="mi">429</span>
                    <span class="p">)</span> <span class="ow">or</span> <span class="s1">&#39;429&#39;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span> <span class="ow">or</span> <span class="s1">&#39;rate limit&#39;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
                    
                    <span class="k">if</span> <span class="n">is_rate_limit</span> <span class="ow">and</span> <span class="n">attempt</span> <span class="o">&lt;</span> <span class="n">max_retries</span><span class="p">:</span>
                        <span class="n">delay</span> <span class="o">=</span> <span class="n">retry_delays</span><span class="p">[</span><span class="n">attempt</span><span class="p">]</span>
                        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Rate limit error (attempt </span><span class="si">{</span><span class="n">attempt</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">max_retries</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">). Retrying in </span><span class="si">{</span><span class="n">delay</span><span class="si">}</span><span class="s2">s...&quot;</span><span class="p">)</span>
                        <span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
                        <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="n">delay</span><span class="p">)</span>
                        <span class="k">continue</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="c1"># Either not a rate limit error, or we&#39;ve exhausted retries</span>
                        <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;OpenAI Vision API error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;OpenAI Vision API call failed: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span> <span class="kn">from</span><span class="w"> </span><span class="nn">e</span>
            
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;OpenAI Vision API error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="c1"># Re-raise the exception instead of returning empty string</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;OpenAI Vision API call failed: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span> <span class="kn">from</span><span class="w"> </span><span class="nn">e</span></div>

    
<div class="viewcode-block" id="OpenAIVisionModelWrapper.generate">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.OpenAIVisionModelWrapper.generate">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">generate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span> <span class="n">config</span><span class="p">:</span> <span class="n">GenerationConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Standard generate method for text-only inputs.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">inputs</span><span class="p">]</span>
            <span class="n">single_input</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">single_input</span> <span class="o">=</span> <span class="kc">False</span>
        
        <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">text_input</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
            <span class="c1"># Create a simple conversation for text-only input</span>
            <span class="n">conversation</span> <span class="o">=</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">text_input</span><span class="p">}]</span>
            <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generate_from_conversation</span><span class="p">(</span><span class="n">conversation</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>
            <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">results</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">single_input</span> <span class="k">else</span> <span class="n">results</span></div>

    
<div class="viewcode-block" id="OpenAIVisionModelWrapper.is_loaded">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.OpenAIVisionModelWrapper.is_loaded">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">is_loaded</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Check if API client is initialized.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_client</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span></div>

    
<div class="viewcode-block" id="OpenAIVisionModelWrapper.unload">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.OpenAIVisionModelWrapper.unload">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">unload</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Cleanup API client.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_client</span> <span class="o">=</span> <span class="kc">None</span></div>
</div>



<div class="viewcode-block" id="GeminiModelWrapper">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.GeminiModelWrapper">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">GeminiModelWrapper</span><span class="p">(</span><span class="n">BaseModelWrapper</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Gemini API-based model wrapper for LLM tasks.&quot;&quot;&quot;</span>
    
<div class="viewcode-block" id="GeminiModelWrapper.__init__">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.GeminiModelWrapper.__init__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_client</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_model</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># Handle max_model_len for API models - store for use in generation config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_model_len</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;max_model_len&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span></div>

        
<div class="viewcode-block" id="GeminiModelWrapper.load">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.GeminiModelWrapper.load">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">load</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Load Gemini API client.&quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="kn">import</span><span class="w"> </span><span class="nn">google.generativeai</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">genai</span>
            <span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
            
            <span class="n">api_key</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;GOOGLE_API_KEY&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">api_key</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;GOOGLE_API_KEY environment variable not set&quot;</span><span class="p">)</span>
            
            <span class="n">genai</span><span class="o">.</span><span class="n">configure</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_model</span> <span class="o">=</span> <span class="n">genai</span><span class="o">.</span><span class="n">GenerativeModel</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Successfully connected to Gemini API for model: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            
        <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span><span class="s2">&quot;Google GenerativeAI package not available. Install with: pip install &#39;marvis[api]&#39;&quot;</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Failed to initialize Gemini client: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">raise</span></div>

    
<div class="viewcode-block" id="GeminiModelWrapper.generate">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.GeminiModelWrapper.generate">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">generate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span> <span class="n">config</span><span class="p">:</span> <span class="n">GenerationConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate text using Gemini API.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_loaded</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Model not loaded&quot;</span><span class="p">)</span>
        
        <span class="c1"># Ensure inputs is a list</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">inputs</span><span class="p">]</span>
            <span class="n">single_input</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">single_input</span> <span class="o">=</span> <span class="kc">False</span>
        
        <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># Override max_new_tokens with max_model_len if provided</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_model_len</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">config</span> <span class="o">=</span> <span class="n">GenerationConfig</span><span class="p">(</span>
                <span class="n">max_new_tokens</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_model_len</span><span class="p">,</span>
                <span class="n">temperature</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">temperature</span><span class="p">,</span>
                <span class="n">top_p</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">top_p</span><span class="p">,</span>
                <span class="n">top_k</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">top_k</span><span class="p">,</span>
                <span class="n">do_sample</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">do_sample</span><span class="p">,</span>
                <span class="n">repetition_penalty</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">repetition_penalty</span><span class="p">,</span>
                <span class="n">stop_tokens</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">stop_tokens</span>
            <span class="p">)</span>
        <span class="n">generation_kwargs</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">to_gemini_kwargs</span><span class="p">()</span>
        
        <span class="k">for</span> <span class="n">text_input</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">generate_content</span><span class="p">(</span>
                    <span class="n">text_input</span><span class="p">,</span>
                    <span class="o">**</span><span class="n">generation_kwargs</span>
                <span class="p">)</span>
                
                <span class="n">generated_text</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">text</span> <span class="k">if</span> <span class="n">response</span><span class="o">.</span><span class="n">text</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span>
                <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">generated_text</span><span class="p">)</span>
                
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Gemini API error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="c1"># Fallback to empty string</span>
                <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">results</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">single_input</span> <span class="k">else</span> <span class="n">results</span></div>

    
<div class="viewcode-block" id="GeminiModelWrapper.is_loaded">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.GeminiModelWrapper.is_loaded">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">is_loaded</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Check if API client is initialized.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span></div>

    
<div class="viewcode-block" id="GeminiModelWrapper.unload">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.GeminiModelWrapper.unload">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">unload</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Cleanup API client.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_model</span> <span class="o">=</span> <span class="kc">None</span></div>
</div>



<div class="viewcode-block" id="GeminiVisionModelWrapper">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.GeminiVisionModelWrapper">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">GeminiVisionModelWrapper</span><span class="p">(</span><span class="n">BaseModelWrapper</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Gemini API-based wrapper for Vision Language Models.&quot;&quot;&quot;</span>
    
<div class="viewcode-block" id="GeminiVisionModelWrapper.__init__">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.GeminiVisionModelWrapper.__init__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_model</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># Handle max_model_len for API models - store for use in generation config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_model_len</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;max_model_len&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span></div>

        
<div class="viewcode-block" id="GeminiVisionModelWrapper.load">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.GeminiVisionModelWrapper.load">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">load</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Load Gemini API client.&quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="kn">import</span><span class="w"> </span><span class="nn">google.generativeai</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">genai</span>
            <span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
            
            <span class="n">api_key</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;GOOGLE_API_KEY&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">api_key</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;GOOGLE_API_KEY environment variable not set&quot;</span><span class="p">)</span>
            
            <span class="n">genai</span><span class="o">.</span><span class="n">configure</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_model</span> <span class="o">=</span> <span class="n">genai</span><span class="o">.</span><span class="n">GenerativeModel</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Successfully connected to Gemini API for VLM: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">model_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            
        <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span><span class="s2">&quot;Google GenerativeAI package not available. Install with: pip install &#39;marvis[api]&#39;&quot;</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Failed to initialize Gemini client: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">raise</span></div>

    
<div class="viewcode-block" id="GeminiVisionModelWrapper.generate_from_conversation">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.GeminiVisionModelWrapper.generate_from_conversation">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">generate_from_conversation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">conversation</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">],</span> <span class="n">config</span><span class="p">:</span> <span class="n">GenerationConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate text from a conversation format with image+text input.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_loaded</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Model not loaded&quot;</span><span class="p">)</span>
        
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Extract content from conversation</span>
            <span class="n">content_parts</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">message</span> <span class="ow">in</span> <span class="n">conversation</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">message</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;content&#39;</span><span class="p">),</span> <span class="nb">list</span><span class="p">):</span>
                    <span class="k">for</span> <span class="n">content_item</span> <span class="ow">in</span> <span class="n">message</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]:</span>
                        <span class="k">if</span> <span class="n">content_item</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;type&#39;</span><span class="p">)</span> <span class="o">==</span> <span class="s1">&#39;text&#39;</span><span class="p">:</span>
                            <span class="n">content_parts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">content_item</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;text&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">))</span>
                        <span class="k">elif</span> <span class="n">content_item</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;type&#39;</span><span class="p">)</span> <span class="o">==</span> <span class="s1">&#39;image&#39;</span><span class="p">:</span>
                            <span class="c1"># Gemini can handle PIL Images directly</span>
                            <span class="n">image</span> <span class="o">=</span> <span class="n">content_item</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;image&#39;</span><span class="p">)</span>
                            <span class="k">if</span> <span class="n">image</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                                <span class="c1"># Debug: log image info</span>
                                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="s1">&#39;size&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="s1">&#39;mode&#39;</span><span class="p">):</span>
                                    <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Adding image to Gemini request: size=</span><span class="si">{</span><span class="n">image</span><span class="o">.</span><span class="n">size</span><span class="si">}</span><span class="s2">, mode=</span><span class="si">{</span><span class="n">image</span><span class="o">.</span><span class="n">mode</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                                <span class="k">else</span><span class="p">:</span>
                                    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Image object doesn&#39;t have expected PIL attributes: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">image</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                                <span class="n">content_parts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
                            <span class="k">else</span><span class="p">:</span>
                                <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;Image is None in conversation content&quot;</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Text-only content</span>
                    <span class="n">content_parts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">message</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;content&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">))</span>
            
            <span class="c1"># Override max_new_tokens with max_model_len if provided</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_model_len</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">config</span> <span class="o">=</span> <span class="n">GenerationConfig</span><span class="p">(</span>
                    <span class="n">max_new_tokens</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_model_len</span><span class="p">,</span>
                    <span class="n">temperature</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">temperature</span><span class="p">,</span>
                    <span class="n">top_p</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">top_p</span><span class="p">,</span>
                    <span class="n">top_k</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">top_k</span><span class="p">,</span>
                    <span class="n">do_sample</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">do_sample</span><span class="p">,</span>
                    <span class="n">repetition_penalty</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">repetition_penalty</span><span class="p">,</span>
                    <span class="n">stop_tokens</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">stop_tokens</span>
                <span class="p">)</span>
            <span class="n">generation_kwargs</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">to_gemini_kwargs</span><span class="p">()</span>
            
            <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">generate_content</span><span class="p">(</span>
                <span class="n">content_parts</span><span class="p">,</span>
                <span class="o">**</span><span class="n">generation_kwargs</span>
            <span class="p">)</span>
            
            <span class="c1"># Handle response more robustly</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">response</span><span class="p">,</span> <span class="s1">&#39;candidates&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">response</span><span class="o">.</span><span class="n">candidates</span><span class="p">:</span>
                <span class="n">candidate</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">candidates</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                
                <span class="c1"># Check finish reason</span>
                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">candidate</span><span class="p">,</span> <span class="s1">&#39;finish_reason&#39;</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">candidate</span><span class="o">.</span><span class="n">finish_reason</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>  <span class="c1"># MAX_TOKENS</span>
                        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Gemini response truncated due to token limit&quot;</span><span class="p">)</span>
                    <span class="k">elif</span> <span class="n">candidate</span><span class="o">.</span><span class="n">finish_reason</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>  <span class="c1"># SAFETY</span>
                        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Gemini response blocked for safety reasons&quot;</span><span class="p">)</span>
                
                <span class="c1"># Try to extract text from parts</span>
                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">candidate</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">candidate</span><span class="o">.</span><span class="n">content</span><span class="p">:</span>
                    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">candidate</span><span class="o">.</span><span class="n">content</span><span class="p">,</span> <span class="s1">&#39;parts&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">candidate</span><span class="o">.</span><span class="n">content</span><span class="o">.</span><span class="n">parts</span><span class="p">:</span>
                        <span class="n">text_parts</span> <span class="o">=</span> <span class="p">[]</span>
                        <span class="k">for</span> <span class="n">part</span> <span class="ow">in</span> <span class="n">candidate</span><span class="o">.</span><span class="n">content</span><span class="o">.</span><span class="n">parts</span><span class="p">:</span>
                            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">part</span><span class="p">,</span> <span class="s1">&#39;text&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">part</span><span class="o">.</span><span class="n">text</span><span class="p">:</span>
                                <span class="n">text_parts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">part</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>
                        <span class="k">if</span> <span class="n">text_parts</span><span class="p">:</span>
                            <span class="k">return</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">text_parts</span><span class="p">)</span>
            
            <span class="c1"># Fallback to response.text if available</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">response</span><span class="o">.</span><span class="n">text</span> <span class="k">if</span> <span class="n">response</span><span class="o">.</span><span class="n">text</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span>
            <span class="k">except</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;Failed to extract text from Gemini response&quot;</span><span class="p">)</span>
                <span class="k">return</span> <span class="s2">&quot;&quot;</span>
            
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Gemini Vision API error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="s2">&quot;&quot;</span></div>

    
<div class="viewcode-block" id="GeminiVisionModelWrapper.generate">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.GeminiVisionModelWrapper.generate">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">generate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span> <span class="n">config</span><span class="p">:</span> <span class="n">GenerationConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Standard generate method for text-only inputs.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">inputs</span><span class="p">]</span>
            <span class="n">single_input</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">single_input</span> <span class="o">=</span> <span class="kc">False</span>
        
        <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">text_input</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
            <span class="c1"># Create a simple conversation for text-only input</span>
            <span class="n">conversation</span> <span class="o">=</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">text_input</span><span class="p">}]</span>
            <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generate_from_conversation</span><span class="p">(</span><span class="n">conversation</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>
            <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">results</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">single_input</span> <span class="k">else</span> <span class="n">results</span></div>

    
<div class="viewcode-block" id="GeminiVisionModelWrapper.is_loaded">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.GeminiVisionModelWrapper.is_loaded">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">is_loaded</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Check if API client is initialized.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span></div>

    
<div class="viewcode-block" id="GeminiVisionModelWrapper.unload">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.GeminiVisionModelWrapper.unload">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">unload</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Cleanup API client.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_model</span> <span class="o">=</span> <span class="kc">None</span></div>
</div>



<div class="viewcode-block" id="ModelLoader">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.ModelLoader">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">ModelLoader</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Central model loading system.&quot;&quot;&quot;</span>
    
<div class="viewcode-block" id="ModelLoader.__init__">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.ModelLoader.__init__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_loaded_models</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">BaseModelWrapper</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span></div>

        
<div class="viewcode-block" id="ModelLoader.load_llm">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.ModelLoader.load_llm">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_llm</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> 
        <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> 
        <span class="n">backend</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BaseModelWrapper</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load a Large Language Model.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            model_name: HuggingFace model name</span>
<span class="sd">            backend: &#39;vllm&#39;, &#39;transformers&#39;, or &#39;auto&#39;</span>
<span class="sd">            device: Device to load model on</span>
<span class="sd">            **kwargs: Additional model loading arguments</span>
<span class="sd">            </span>
<span class="sd">        Returns:</span>
<span class="sd">            Loaded model wrapper</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">cache_key</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">backend</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span>
        
        <span class="c1"># Return cached model if available</span>
        <span class="k">if</span> <span class="n">cache_key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loaded_models</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Using cached model: </span><span class="si">{</span><span class="n">cache_key</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loaded_models</span><span class="p">[</span><span class="n">cache_key</span><span class="p">]</span>
        
        <span class="c1"># Determine backend</span>
        <span class="k">if</span> <span class="n">backend</span> <span class="o">==</span> <span class="s2">&quot;auto&quot;</span><span class="p">:</span>
            <span class="c1"># Check for API models first</span>
            <span class="n">openai_llm_models</span> <span class="o">=</span> <span class="p">[</span>
                <span class="s2">&quot;gpt-4.1&quot;</span><span class="p">,</span> <span class="s2">&quot;gpt-4.1-mini&quot;</span><span class="p">,</span> <span class="s2">&quot;gpt-4.1-nano&quot;</span><span class="p">,</span>
                <span class="s2">&quot;gpt-4o&quot;</span><span class="p">,</span> <span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">,</span> <span class="s2">&quot;o3&quot;</span><span class="p">,</span> <span class="s2">&quot;o4-mini&quot;</span>
            <span class="p">]</span>
            <span class="n">gemini_models</span> <span class="o">=</span> <span class="p">[</span>
                <span class="s2">&quot;gemini-2.5-pro&quot;</span><span class="p">,</span> <span class="s2">&quot;gemini-2.5-flash&quot;</span><span class="p">,</span> <span class="s2">&quot;gemini-2.5-flash-lite&quot;</span><span class="p">,</span>
                <span class="s2">&quot;gemini-2.0-flash&quot;</span><span class="p">,</span> <span class="s2">&quot;gemini-2.0-pro-experimental&quot;</span>
            <span class="p">]</span>
            
            <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">model</span> <span class="ow">in</span> <span class="n">model_name</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">openai_llm_models</span><span class="p">):</span>
                <span class="n">backend</span> <span class="o">=</span> <span class="s2">&quot;openai&quot;</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Auto-selected OpenAI backend for LLM: </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">any</span><span class="p">(</span><span class="n">model</span> <span class="ow">in</span> <span class="n">model_name</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">gemini_models</span><span class="p">):</span>
                <span class="n">backend</span> <span class="o">=</span> <span class="s2">&quot;gemini&quot;</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Auto-selected Gemini backend for LLM: </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Prefer VLLM for local LLMs if available</span>
                <span class="n">backend</span> <span class="o">=</span> <span class="s2">&quot;vllm&quot;</span> <span class="k">if</span> <span class="n">VLLM_AVAILABLE</span> <span class="k">else</span> <span class="s2">&quot;transformers&quot;</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Auto-selected </span><span class="si">{</span><span class="n">backend</span><span class="si">}</span><span class="s2"> backend for local LLM: </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        
        <span class="c1"># Create wrapper</span>
        <span class="k">if</span> <span class="n">backend</span> <span class="o">==</span> <span class="s2">&quot;openai&quot;</span><span class="p">:</span>
            <span class="n">wrapper</span> <span class="o">=</span> <span class="n">OpenAIModelWrapper</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">backend</span> <span class="o">==</span> <span class="s2">&quot;gemini&quot;</span><span class="p">:</span>
            <span class="n">wrapper</span> <span class="o">=</span> <span class="n">GeminiModelWrapper</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">backend</span> <span class="o">==</span> <span class="s2">&quot;vllm&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">VLLM_AVAILABLE</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;VLLM not available, falling back to transformers&quot;</span><span class="p">)</span>
                <span class="n">backend</span> <span class="o">=</span> <span class="s2">&quot;transformers&quot;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">wrapper</span> <span class="o">=</span> <span class="n">VLLMModelWrapper</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">backend</span> <span class="o">==</span> <span class="s2">&quot;transformers&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">TRANSFORMERS_AVAILABLE</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span><span class="s2">&quot;Transformers not available&quot;</span><span class="p">)</span>
            <span class="n">wrapper</span> <span class="o">=</span> <span class="n">TransformersModelWrapper</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">backend</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;openai&quot;</span><span class="p">,</span> <span class="s2">&quot;gemini&quot;</span><span class="p">,</span> <span class="s2">&quot;vllm&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown backend: </span><span class="si">{</span><span class="n">backend</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        
        <span class="c1"># Load the model</span>
        <span class="n">wrapper</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>
        
        <span class="c1"># Cache and return</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_loaded_models</span><span class="p">[</span><span class="n">cache_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">wrapper</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loaded and cached model: </span><span class="si">{</span><span class="n">cache_key</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">wrapper</span></div>

    
<div class="viewcode-block" id="ModelLoader.load_vlm">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.ModelLoader.load_vlm">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_vlm</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> 
        <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> 
        <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span>
        <span class="n">backend</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">VisionLanguageModelWrapper</span><span class="p">,</span> <span class="n">VLLMVisionModelWrapper</span><span class="p">,</span> <span class="n">OpenAIVisionModelWrapper</span><span class="p">,</span> <span class="n">GeminiVisionModelWrapper</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load a Vision Language Model.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            model_name: HuggingFace model name</span>
<span class="sd">            device: Device to load model on</span>
<span class="sd">            backend: Backend to use (&quot;auto&quot;, &quot;vllm&quot;, &quot;transformers&quot;)</span>
<span class="sd">            **kwargs: Additional model loading arguments</span>
<span class="sd">            </span>
<span class="sd">        Returns:</span>
<span class="sd">            Loaded VLM wrapper</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Determine backend</span>
        <span class="k">if</span> <span class="n">backend</span> <span class="o">==</span> <span class="s2">&quot;auto&quot;</span><span class="p">:</span>
            <span class="c1"># Check for API VLM models first</span>
            <span class="n">openai_vlm_models</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;gpt-4.1&quot;</span><span class="p">,</span> <span class="s2">&quot;gpt-4o&quot;</span><span class="p">]</span>  <span class="c1"># Both support vision</span>
            <span class="n">gemini_vlm_models</span> <span class="o">=</span> <span class="p">[</span>
                <span class="s2">&quot;gemini-2.5-pro&quot;</span><span class="p">,</span> <span class="s2">&quot;gemini-2.5-flash&quot;</span><span class="p">,</span> <span class="s2">&quot;gemini-2.5-flash-lite&quot;</span><span class="p">,</span>
                <span class="s2">&quot;gemini-2.0-flash&quot;</span><span class="p">,</span> <span class="s2">&quot;gemini-2.0-pro-experimental&quot;</span>
            <span class="p">]</span>
            
            <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">model</span> <span class="ow">in</span> <span class="n">model_name</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">openai_vlm_models</span><span class="p">):</span>
                <span class="n">backend</span> <span class="o">=</span> <span class="s2">&quot;openai&quot;</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Auto-selected OpenAI backend for VLM: </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">any</span><span class="p">(</span><span class="n">model</span> <span class="ow">in</span> <span class="n">model_name</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">gemini_vlm_models</span><span class="p">):</span>
                <span class="n">backend</span> <span class="o">=</span> <span class="s2">&quot;gemini&quot;</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Auto-selected Gemini backend for VLM: </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Check if model supports VLLM multimodal</span>
                <span class="n">vlm_supported_models</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="s2">&quot;Qwen/Qwen2.5-VL&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;Qwen/Qwen2-VL&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;llava-hf/llava&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;TIGER-Lab/Mantis&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;microsoft/Phi-3.5-vision&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;mistral-community/pixtral&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;allenai/Molmo&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;meta-llama/Llama-3.2-11B-Vision&quot;</span>
                <span class="p">]</span>
                
                <span class="c1"># Check if model name matches any supported pattern</span>
                <span class="n">supports_vllm</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="n">pattern</span> <span class="ow">in</span> <span class="n">model_name</span> <span class="k">for</span> <span class="n">pattern</span> <span class="ow">in</span> <span class="n">vlm_supported_models</span><span class="p">)</span>
                
                <span class="k">if</span> <span class="n">supports_vllm</span> <span class="ow">and</span> <span class="n">VLLM_AVAILABLE</span><span class="p">:</span>
                    <span class="n">backend</span> <span class="o">=</span> <span class="s2">&quot;vllm&quot;</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Auto-selected VLLM backend for VLM: </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">backend</span> <span class="o">=</span> <span class="s2">&quot;transformers&quot;</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Auto-selected transformers backend for VLM: </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        
        <span class="n">cache_key</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">_vlm_</span><span class="si">{</span><span class="n">backend</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span>
        
        <span class="c1"># Return cached model if available</span>
        <span class="k">if</span> <span class="n">cache_key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loaded_models</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Using cached VLM: </span><span class="si">{</span><span class="n">cache_key</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loaded_models</span><span class="p">[</span><span class="n">cache_key</span><span class="p">]</span>
        
        <span class="c1"># Create appropriate wrapper based on backend</span>
        <span class="k">if</span> <span class="n">backend</span> <span class="o">==</span> <span class="s2">&quot;openai&quot;</span><span class="p">:</span>
            <span class="n">wrapper</span> <span class="o">=</span> <span class="n">OpenAIVisionModelWrapper</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">backend</span> <span class="o">==</span> <span class="s2">&quot;gemini&quot;</span><span class="p">:</span>
            <span class="n">wrapper</span> <span class="o">=</span> <span class="n">GeminiVisionModelWrapper</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">backend</span> <span class="o">==</span> <span class="s2">&quot;vllm&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">VLLM_AVAILABLE</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;VLLM not available, falling back to transformers&quot;</span><span class="p">)</span>
                <span class="n">backend</span> <span class="o">=</span> <span class="s2">&quot;transformers&quot;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">wrapper</span> <span class="o">=</span> <span class="n">VLLMVisionModelWrapper</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">backend</span> <span class="o">==</span> <span class="s2">&quot;transformers&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">TRANSFORMERS_AVAILABLE</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span><span class="s2">&quot;Transformers not available&quot;</span><span class="p">)</span>
            <span class="n">wrapper</span> <span class="o">=</span> <span class="n">VisionLanguageModelWrapper</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">backend</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;openai&quot;</span><span class="p">,</span> <span class="s2">&quot;gemini&quot;</span><span class="p">,</span> <span class="s2">&quot;vllm&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown backend: </span><span class="si">{</span><span class="n">backend</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        
        <span class="c1"># Load the model</span>
        <span class="n">wrapper</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>
        
        <span class="c1"># Cache and return</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_loaded_models</span><span class="p">[</span><span class="n">cache_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">wrapper</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loaded and cached VLM: </span><span class="si">{</span><span class="n">cache_key</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">wrapper</span></div>

    
<div class="viewcode-block" id="ModelLoader.unload_model">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.ModelLoader.unload_model">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">unload_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">backend</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Unload a specific model.&quot;&quot;&quot;</span>
        <span class="c1"># If specific cache key provided</span>
        <span class="k">if</span> <span class="n">backend</span> <span class="ow">and</span> <span class="n">device</span><span class="p">:</span>
            <span class="n">cache_key</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">backend</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="k">if</span> <span class="n">cache_key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loaded_models</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_loaded_models</span><span class="p">[</span><span class="n">cache_key</span><span class="p">]</span><span class="o">.</span><span class="n">unload</span><span class="p">()</span>
                <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loaded_models</span><span class="p">[</span><span class="n">cache_key</span><span class="p">]</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unloaded model: </span><span class="si">{</span><span class="n">cache_key</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Unload all variants of this model</span>
            <span class="n">keys_to_remove</span> <span class="o">=</span> <span class="p">[</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loaded_models</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">model_name</span><span class="p">)]</span>
            <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">keys_to_remove</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_loaded_models</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">unload</span><span class="p">()</span>
                <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loaded_models</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unloaded model: </span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="ModelLoader.unload_all">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.ModelLoader.unload_all">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">unload_all</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Unload all models.&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">wrapper</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loaded_models</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
            <span class="n">wrapper</span><span class="o">.</span><span class="n">unload</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_loaded_models</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Unloaded all models&quot;</span><span class="p">)</span></div>

    
<div class="viewcode-block" id="ModelLoader.list_loaded_models">
<a class="viewcode-back" href="../../../api-reference/marvis.utils.html#marvis.utils.model_loader.ModelLoader.list_loaded_models">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">list_loaded_models</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;List all currently loaded models.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_loaded_models</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span></div>
</div>



<span class="c1"># Global model loader instance</span>
<span class="n">model_loader</span> <span class="o">=</span> <span class="n">ModelLoader</span><span class="p">()</span>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, MARVIS Development Team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>