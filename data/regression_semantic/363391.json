{
  "dataset_name": "jigsaw-unintended-bias-in-toxicity",
  "description": "A large-scale dataset of online comments labeled for toxicity and identity mentions, created to help develop machine learning models that can detect toxic comments while minimizing unintended bias against mentions of frequently attacked identities. The dataset contains comments from the Civil Comments platform with human-annotated labels for toxicity and various identity attributes.",
  "original_source": {
    "creator": "Jigsaw (Google)",
    "institution": "Google/Jigsaw in partnership with Civil Comments",
    "date": "2019",
    "publication": "Nuanced Metrics for Measuring Unintended Bias with Real Data for Text Classification (Borkan et al., 2019, WWW Conference)"
  },
  "columns": [
    {
      "name": "comment_text",
      "semantic_description": "The raw text of the online comment",
      "data_type": "text"
    },
    {
      "name": "toxicity",
      "semantic_description": "The primary toxicity score indicating how toxic the comment is, based on human rater consensus",
      "data_type": "float between 0 and 1"
    },
    {
      "name": "severe_toxicity",
      "semantic_description": "Score for comments that are considered very hateful/toxic",
      "data_type": "float between 0 and 1"
    },
    {
      "name": "identity_attack",
      "semantic_description": "Score indicating whether the comment attacks someone's identity (e.g., race, religion, etc.)",
      "data_type": "float between 0 and 1"
    },
    {
      "name": "insult",
      "semantic_description": "Score indicating whether the comment is an insult",
      "data_type": "float between 0 and 1"
    },
    {
      "name": "threat",
      "semantic_description": "Score indicating whether the comment contains threats",
      "data_type": "float between 0 and 1"
    }
  ],
  "target_description": {
    "name": "toxicity",
    "meaning": "A measure of how toxic, rude, disrespectful, or unreasonable a comment is likely to make someone leave a discussion",
    "units": "Probability score",
    "range": "0.0 (not toxic) to 1.0 (very toxic)"
  },
  "dataset_history": "The dataset was created from the Civil Comments platform, which hosted comments for independent news sites from 2015 until 2017. When the platform shut down, they made their ~2M comments public for research. Jigsaw extended this dataset by adding detailed annotations for toxicity and identity mentions to help develop better content moderation systems that minimize unintended biases against frequently targeted groups.",
  "inference_notes": "Important considerations include:\n1. Annotator bias: Labels are based on human judgments which may contain inherent biases\n2. Cultural context: Toxicity can be culturally dependent and subjective\n3. Class imbalance: Toxic comments are relatively rare in the dataset\n4. Identity term bias: Comments containing certain identity terms may be disproportionately labeled as toxic\n5. Temporal relevance: The data reflects online discourse from 2015-2017 and may not fully represent current patterns\n6. Inter-rater reliability: Multiple annotators may disagree on toxicity ratings\n7. The dataset was specifically designed to address and measure unintended bias in toxicity detection systems",
  "_metadata": {
    "task_id": 363391,
    "generated_at": "2025-06-22T08:29:47.968978",
    "generator": "generate_openml_semantic_info.py",
    "claude_model": "claude-3-5-sonnet-20241022"
  }
}