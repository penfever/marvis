{
  "dataset_name": "google_qa_answer_type_reason_explanation",
  "description": "This dataset is derived from the Google QUEST Q&A Labeling challenge, focusing on measuring subjective aspects of question-answering quality. It contains features extracted from question-answer pairs from websites like Stack Exchange, analyzing how well answers address questions across different domains. The specific target variable focuses on classifying the reasoning/explanation type provided in answers.",
  "original_source": {
    "creator": "Google Research & Kaggle",
    "institution": "Google (Crowdsource team)",
    "date": "2019",
    "publication": "Originally released as part of Kaggle competition 'Google QUEST Q&A Labeling Challenge'"
  },
  "columns": [
    {
      "name": "question_title",
      "semantic_description": "The main title/headline of the question as posted on the Q&A platform",
      "data_type": "text"
    },
    {
      "name": "question_body",
      "semantic_description": "The detailed content/description of the question",
      "data_type": "text"
    },
    {
      "name": "answer_content",
      "semantic_description": "The full text of the answer provided",
      "data_type": "text"
    },
    {
      "name": "category",
      "semantic_description": "The topic/domain category of the question (e.g., technology, science, etc.)",
      "data_type": "categorical"
    },
    {
      "name": "encoded_features_0-37",
      "semantic_description": "BERT/NLP-derived numerical features encoding semantic aspects of the question-answer pair relationship",
      "data_type": "numeric, normalized vectors"
    }
  ],
  "target_description": {
    "name": "answer_type_reason_explanation",
    "meaning": "Measures the degree to which the answer provides reasoning and explanations rather than just facts or statements",
    "units": "Normalized score between 0 and 1",
    "range": "0.0 (no explanation/reasoning) to 1.0 (comprehensive explanation with clear reasoning)"
  },
  "dataset_history": "The dataset was created as part of Google's efforts to improve automated assessment of Q&A quality on platforms like Stack Exchange. It was first released through a Kaggle competition in 2019 where participants were challenged to develop models that could evaluate multiple quality aspects of question-answer pairs. The data was labeled by Google's Crowdsource contributors using a specialized rubric for consistency.",
  "inference_notes": "Important considerations include:\n1. Subjective nature of ratings - even human raters may disagree on scores\n2. Domain expertise affects interpretation - technical answers may appear more/less explanatory depending on reader expertise\n3. Cultural and language biases may be present due to English-language focus\n4. Features are pre-processed using BERT, so some original text information may be lost\n5. Category information should be considered alongside text features for best results\n6. Scores are relative rather than absolute measures of explanation quality",
  "_metadata": {
    "task_id": 363370,
    "generated_at": "2025-06-22T08:25:58.556214",
    "generator": "generate_openml_semantic_info.py",
    "claude_model": "claude-3-5-sonnet-20241022"
  }
}