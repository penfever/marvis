{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† MARVIS Getting Started Guide\n",
    "\n",
    "Welcome to **MARVIS** (Modality Adaptive Reasoning over VISualizations)! This notebook will guide you through using MARVIS to make predictions on **tabular**, **image**, and **audio** data using the unified `MarvisTsneClassifier` interface.\n",
    "\n",
    "## What is MARVIS?\n",
    "\n",
    "MARVIS is a novel approach that:\n",
    "1. **Embeds** your data into high-dimensional representations\n",
    "2. **Visualizes** the data using t-SNE or other dimensionality reduction techniques\n",
    "3. **Reasons** about the visualization using Vision Language Models (VLMs)\n",
    "4. **Predicts** class labels based on visual patterns\n",
    "\n",
    "Let's start by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Add MARVIS to Python path\n",
    "marvis_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(marvis_root))\n",
    "\n",
    "# Import MARVIS modules\n",
    "from marvis.models.marvis_tsne import MarvisTsneClassifier\n",
    "from marvis.utils.audio_utils import create_synthetic_audio\n",
    "from sklearn.datasets import make_classification, load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display, Image as IPImage, Audio\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üéâ MARVIS libraries imported successfully!\")\n",
    "print(f\"üìÅ MARVIS root directory: {marvis_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Part 1: Tabular Data Classification\n",
    "\n",
    "Let's start with tabular data - the most common use case. We'll create a synthetic dataset and show how MARVIS can classify it using visual reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic tabular dataset\n",
    "print(\"üìä Creating synthetic tabular dataset...\")\n",
    "\n",
    "X_tabular, y_tabular = make_classification(\n",
    "    n_samples=200,\n",
    "    n_features=10,\n",
    "    n_classes=3,\n",
    "    n_informative=8,\n",
    "    n_redundant=1,\n",
    "    n_clusters_per_class=1,\n",
    "    class_sep=1.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split into train/test\n",
    "X_train_tab, X_test_tab, y_train_tab, y_test_tab = train_test_split(\n",
    "    X_tabular, y_tabular, test_size=0.3, random_state=42, stratify=y_tabular\n",
    ")\n",
    "\n",
    "class_names_tab = [\"Class A\", \"Class B\", \"Class C\"]\n",
    "\n",
    "print(f\"‚úÖ Dataset created:\")\n",
    "print(f\"   ‚Ä¢ Training samples: {len(X_train_tab)}\")\n",
    "print(f\"   ‚Ä¢ Test samples: {len(X_test_tab)}\")\n",
    "print(f\"   ‚Ä¢ Features: {X_tabular.shape[1]}\")\n",
    "print(f\"   ‚Ä¢ Classes: {len(class_names_tab)}\")\n",
    "print(f\"   ‚Ä¢ Class distribution: {np.bincount(y_tabular)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MARVIS classifier for tabular data\n",
    "print(\"üîß Initializing MARVIS t-SNE classifier for tabular data...\")\n",
    "\n",
    "tabular_classifier = MarvisTsneClassifier(\n",
    "    modality=\"tabular\",\n",
    "    vlm_model_id=\"Qwen/Qwen2.5-VL-3B-Instruct\",  # Fast and efficient VLM\n",
    "    tsne_perplexity=15,\n",
    "    tsne_max_iter=500,\n",
    "    use_3d=False,\n",
    "    use_knn_connections=True,\n",
    "    nn_k=5,\n",
    "    max_vlm_image_size=800,\n",
    "    image_dpi=100,\n",
    "    use_semantic_names=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Tabular classifier initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Fit the classifier\nprint(\"üèãÔ∏è Training MARVIS on tabular data...\")\n\ntabular_classifier.fit(\n    X_train_tab, \n    y_train_tab, \n    X_test_tab, \n    class_names=class_names_tab,\n    task_type='classification'  # Explicitly specify task type for synthetic data\n)\n\nprint(\"‚úÖ Tabular classifier trained successfully!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions and evaluate\n",
    "print(\"üîÆ Making predictions on tabular test data...\")\n",
    "\n",
    "# Create temporary directory for outputs\n",
    "temp_dir = tempfile.mkdtemp(prefix='marvis_demo_')\n",
    "print(f\"üìÅ Saving outputs to: {temp_dir}\")\n",
    "\n",
    "try:\n",
    "    # Evaluate with detailed results and save outputs\n",
    "    tabular_results = tabular_classifier.predict_with_detailed_results(\n",
    "        X_test_tab[:10],  # Use first 10 test samples for demo\n",
    "        y_test_tab[:10],\n",
    "        save_visualizations=True,\n",
    "        output_dir=temp_dir\n",
    "    )\n",
    "    \n",
    "    print(\"\\nüìà Tabular Classification Results:\")\n",
    "    accuracy = tabular_results.get('accuracy', 'N/A')\n",
    "    total_samples = len(X_test_tab[:10])\n",
    "    completed_samples = tabular_results.get('completed_samples', total_samples)\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Accuracy: {accuracy}\")\n",
    "    print(f\"   ‚Ä¢ Completed samples: {completed_samples}\")\n",
    "    \n",
    "    # Display some predictions\n",
    "    predictions = tabular_results.get('predictions', [])\n",
    "    if predictions:\n",
    "        print(\"\\nüéØ Sample Predictions:\")\n",
    "        for i, pred in enumerate(predictions[:3]):\n",
    "            predicted_class = pred if isinstance(pred, str) else class_names_tab[pred] if pred < len(class_names_tab) else f\"Class {pred}\"\n",
    "            true_class = class_names_tab[y_test_tab[i]]\n",
    "            print(f\"   Sample {i+1}: Predicted='{predicted_class}', True='{true_class}'\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during evaluation: {e}\")\n",
    "    print(\"This might be due to VLM API limitations. The classifier is still trained successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the visualization\n",
    "print(\"üñºÔ∏è Displaying t-SNE visualization for tabular data...\")\n",
    "\n",
    "# Look for visualization files in the temp directory\n",
    "viz_files = list(Path(temp_dir).glob(\"*.png\"))\n",
    "\n",
    "if viz_files:\n",
    "    # Display the most recent visualization\n",
    "    latest_viz = max(viz_files, key=lambda p: p.stat().st_mtime)\n",
    "    print(f\"üìä Found visualization: {latest_viz.name}\")\n",
    "    \n",
    "    # Display using IPython\n",
    "    display(IPImage(filename=str(latest_viz), width=600))\n",
    "    \n",
    "    print(\"\\nüé® Visualization Features:\")\n",
    "    print(\"   ‚Ä¢ Colored points represent training data by class\")\n",
    "    print(\"   ‚Ä¢ Gray squares represent test data\")\n",
    "    print(\"   ‚Ä¢ KNN connections show nearest neighbors\")\n",
    "    print(\"   ‚Ä¢ The VLM analyzes this visualization to make predictions\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No visualization files found. This might be due to VLM API limitations.\")\n",
    "    print(\"The visualization would normally show:\")\n",
    "    print(\"   ‚Ä¢ t-SNE projection of high-dimensional tabular data\")\n",
    "    print(\"   ‚Ä¢ Color-coded classes for visual pattern recognition\")\n",
    "    print(\"   ‚Ä¢ KNN connections between similar data points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üñºÔ∏è Part 2: Image Data Classification\n",
    "\n",
    "Now let's try MARVIS with image data. We'll use the digits dataset as an example of computer vision tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load digits dataset (images)\n",
    "print(\"üñºÔ∏è Loading digits dataset (image data)...\")\n",
    "\n",
    "digits = load_digits()\n",
    "X_images = digits.data  # 8x8 pixel images flattened to 64 features\n",
    "y_images = digits.target\n",
    "\n",
    "# Use only first 3 classes for simplicity\n",
    "mask = y_images < 3\n",
    "X_images = X_images[mask]\n",
    "y_images = y_images[mask]\n",
    "\n",
    "# Split into train/test\n",
    "X_train_img, X_test_img, y_train_img, y_test_img = train_test_split(\n",
    "    X_images, y_images, test_size=0.3, random_state=42, stratify=y_images\n",
    ")\n",
    "\n",
    "class_names_img = [\"Digit 0\", \"Digit 1\", \"Digit 2\"]\n",
    "\n",
    "print(f\"‚úÖ Digits dataset loaded:\")\n",
    "print(f\"   ‚Ä¢ Training samples: {len(X_train_img)}\")\n",
    "print(f\"   ‚Ä¢ Test samples: {len(X_test_img)}\")\n",
    "print(f\"   ‚Ä¢ Image size: 8x8 pixels (64 features)\")\n",
    "print(f\"   ‚Ä¢ Classes: {class_names_img}\")\n",
    "\n",
    "# Visualize some sample digits\n",
    "fig, axes = plt.subplots(1, 6, figsize=(12, 2))\n",
    "for i in range(6):\n",
    "    axes[i].imshow(X_train_img[i].reshape(8, 8), cmap='gray')\n",
    "    axes[i].set_title(f\"Class: {class_names_img[y_train_img[i]]}\")\n",
    "    axes[i].axis('off')\n",
    "plt.suptitle(\"Sample Digit Images\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MARVIS classifier for image data\n",
    "print(\"üîß Initializing MARVIS t-SNE classifier for image data...\")\n",
    "\n",
    "image_classifier = MarvisTsneClassifier(\n",
    "    modality=\"vision\",  # Specify vision modality for image data\n",
    "    vlm_model_id=\"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
    "    tsne_perplexity=20,\n",
    "    tsne_max_iter=500,\n",
    "    use_3d=False,\n",
    "    use_knn_connections=True,\n",
    "    nn_k=5,\n",
    "    max_vlm_image_size=800,\n",
    "    use_semantic_names=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Image classifier initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Fit the image classifier\nprint(\"üèãÔ∏è Training MARVIS on image data...\")\n\nimage_classifier.fit(\n    X_train_img,\n    y_train_img,\n    X_test_img[:20],  # Use subset for demo\n    class_names=class_names_img,\n    task_type='classification'  # Explicitly specify task type\n)\n\nprint(\"‚úÖ Image classifier trained successfully!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on image data\n",
    "print(\"üîÆ Making predictions on image test data...\")\n",
    "\n",
    "# Create new temp directory for image outputs\n",
    "temp_dir_img = tempfile.mkdtemp(prefix='marvis_image_demo_')\n",
    "print(f\"üìÅ Saving image outputs to: {temp_dir_img}\")\n",
    "\n",
    "try:\n",
    "    # Evaluate on a small subset\n",
    "    image_results = image_classifier.predict_with_detailed_results(\n",
    "        X_test_img[:8],  # Use first 8 test samples\n",
    "        y_test_img[:8],\n",
    "        save_visualizations=True,\n",
    "        output_dir=temp_dir_img\n",
    "    )\n",
    "    \n",
    "    print(\"\\nüìà Image Classification Results:\")\n",
    "    accuracy = image_results.get('accuracy', 'N/A')\n",
    "    completed_samples = image_results.get('completed_samples', len(X_test_img[:8]))\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Accuracy: {accuracy}\")\n",
    "    print(f\"   ‚Ä¢ Completed samples: {completed_samples}\")\n",
    "    \n",
    "    # Display some predictions\n",
    "    predictions = image_results.get('predictions', [])\n",
    "    if predictions:\n",
    "        print(\"\\nüéØ Sample Image Predictions:\")\n",
    "        for i, pred in enumerate(predictions[:3]):\n",
    "            predicted_class = pred if isinstance(pred, str) else class_names_img[pred] if pred < len(class_names_img) else f\"Class {pred}\"\n",
    "            true_class = class_names_img[y_test_img[i]]\n",
    "            print(f\"   Sample {i+1}: Predicted='{predicted_class}', True='{true_class}'\")\n",
    "                \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during image evaluation: {e}\")\n",
    "    print(\"The classifier is trained, but prediction may require VLM API access.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the image visualization\n",
    "print(\"üñºÔ∏è Displaying t-SNE visualization for image data...\")\n",
    "\n",
    "# Look for visualization files\n",
    "viz_files_img = list(Path(temp_dir_img).glob(\"*.png\"))\n",
    "\n",
    "if viz_files_img:\n",
    "    latest_viz_img = max(viz_files_img, key=lambda p: p.stat().st_mtime)\n",
    "    print(f\"üìä Found image visualization: {latest_viz_img.name}\")\n",
    "    \n",
    "    display(IPImage(filename=str(latest_viz_img), width=600))\n",
    "    \n",
    "    print(\"\\nüé® Image Visualization Features:\")\n",
    "    print(\"   ‚Ä¢ Each point represents a digit image in 2D t-SNE space\")\n",
    "    print(\"   ‚Ä¢ Colors distinguish different digit classes (0, 1, 2)\")\n",
    "    print(\"   ‚Ä¢ Similar digits cluster together in the visualization\")\n",
    "    print(\"   ‚Ä¢ VLM identifies patterns to classify new digits\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No image visualization found. This would show:\")\n",
    "    print(\"   ‚Ä¢ t-SNE embedding of 64-dimensional digit features\")\n",
    "    print(\"   ‚Ä¢ Clear clustering of similar digit patterns\")\n",
    "    print(\"   ‚Ä¢ Visual separation between different digit classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéµ Part 3: Audio Data Classification\n",
    "\n",
    "Finally, let's explore MARVIS's capabilities with audio data. We'll create synthetic audio samples with different frequencies to demonstrate audio classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic audio dataset\n",
    "print(\"üéµ Creating synthetic audio dataset...\")\n",
    "\n",
    "# Create temporary directory for audio files\n",
    "audio_temp_dir = tempfile.mkdtemp(prefix='clam_audio_')\n",
    "print(f\"üìÅ Creating audio files in: {audio_temp_dir}\")\n",
    "\n",
    "# Generate synthetic audio samples\n",
    "audio_paths = []\n",
    "audio_labels = []\n",
    "class_names_audio = [\"Low Tone\", \"Mid Tone\", \"High Tone\"]\n",
    "\n",
    "# Create samples for each class\n",
    "samples_per_class = 15\n",
    "duration = 2.0  # 2 seconds\n",
    "sample_rate = 16000\n",
    "\n",
    "try:\n",
    "    import soundfile as sf\n",
    "    \n",
    "    for class_idx, class_name in enumerate(class_names_audio):\n",
    "        base_freq = 200 + class_idx * 300  # 200Hz, 500Hz, 800Hz\n",
    "        \n",
    "        for sample_idx in range(samples_per_class):\n",
    "            # Add some variation to the frequency\n",
    "            freq_variation = np.random.uniform(-50, 50)\n",
    "            frequency = base_freq + freq_variation\n",
    "            \n",
    "            # Create synthetic audio\n",
    "            audio = create_synthetic_audio(\n",
    "                frequency=frequency,\n",
    "                duration=duration,\n",
    "                sample_rate=sample_rate,\n",
    "                noise_level=0.1\n",
    "            )\n",
    "            \n",
    "            # Save to file\n",
    "            audio_path = Path(audio_temp_dir) / f\"{class_name.lower().replace(' ', '_')}_{sample_idx:02d}.wav\"\n",
    "            sf.write(str(audio_path), audio, sample_rate)\n",
    "            \n",
    "            audio_paths.append(str(audio_path))\n",
    "            audio_labels.append(class_idx)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    audio_labels = np.array(audio_labels)\n",
    "    \n",
    "    # Split into train/test\n",
    "    audio_train_paths, audio_test_paths, y_train_audio, y_test_audio = train_test_split(\n",
    "        audio_paths, audio_labels, test_size=0.3, random_state=42, stratify=audio_labels\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Audio dataset created:\")\n",
    "    print(f\"   ‚Ä¢ Training samples: {len(audio_train_paths)}\")\n",
    "    print(f\"   ‚Ä¢ Test samples: {len(audio_test_paths)}\")\n",
    "    print(f\"   ‚Ä¢ Duration: {duration}s per sample\")\n",
    "    print(f\"   ‚Ä¢ Classes: {class_names_audio}\")\n",
    "    print(f\"   ‚Ä¢ Frequencies: ~200Hz, ~500Hz, ~800Hz\")\n",
    "    \n",
    "    # Display audio waveforms\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(12, 8))\n",
    "    \n",
    "    for class_idx in range(3):\n",
    "        # Load and plot first sample of each class\n",
    "        sample_path = [p for p, l in zip(audio_train_paths, y_train_audio) if l == class_idx][0]\n",
    "        audio_data, sr = sf.read(sample_path)\n",
    "        \n",
    "        time_axis = np.arange(len(audio_data)) / sr\n",
    "        axes[class_idx].plot(time_axis[:1000], audio_data[:1000])  # Plot first 1000 samples\n",
    "        axes[class_idx].set_title(f\"{class_names_audio[class_idx]} - Sample Waveform\")\n",
    "        axes[class_idx].set_xlabel(\"Time (s)\")\n",
    "        axes[class_idx].set_ylabel(\"Amplitude\")\n",
    "        axes[class_idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Play a sample audio (first test sample)\n",
    "    if audio_test_paths:\n",
    "        sample_audio, sample_sr = sf.read(audio_test_paths[0])\n",
    "        print(f\"\\nüéß Playing sample audio ({class_names_audio[y_test_audio[0]]})...\")\n",
    "        display(Audio(sample_audio, rate=sample_sr))\n",
    "\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è soundfile not available. Skipping audio file creation.\")\n",
    "    print(\"Audio classification would work with real audio files.\")\n",
    "    audio_available = False\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error creating audio files: {e}\")\n",
    "    audio_available = False\n",
    "else:\n",
    "    audio_available = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if audio_available:\n",
    "    # Initialize MARVIS classifier for audio data\n",
    "    print(\"üîß Initializing MARVIS audio classifier...\")\n",
    "    \n",
    "    try:\n",
    "        audio_classifier = MarvisTsneClassifier(\n",
    "            modality=\"audio\",\n",
    "            whisper_model=\"tiny\",  # Fast Whisper model for embeddings\n",
    "            vlm_model_id=\"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
    "            tsne_perplexity=10,\n",
    "            tsne_max_iter=300,\n",
    "            use_3d=False,\n",
    "            use_knn_connections=True,\n",
    "            nn_k=3,\n",
    "            max_vlm_image_size=800,\n",
    "            device=\"auto\",  # Use auto device detection\n",
    "            seed=42\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Audio classifier initialized!\")\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(f\"‚ö†Ô∏è Audio classifier not available: {e}\")\n",
    "        print(\"This requires additional audio dependencies.\")\n",
    "        audio_classifier = None\n",
    "else:\n",
    "    audio_classifier = None"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "if audio_available and audio_classifier is not None:\n    # Fit the audio classifier\n    print(\"üèãÔ∏è Training MARVIS on audio data...\")\n    \n    try:\n        audio_classifier.fit(\n            audio_train_paths,\n            y_train_audio,\n            audio_test_paths[:10],  # Use subset for demo\n            class_names=class_names_audio,\n            task_type='classification'  # Explicitly specify task type\n        )\n        \n        print(\"‚úÖ Audio classifier trained successfully!\")\n        \n        # Make predictions\n        print(\"\\nüîÆ Making predictions on audio test data...\")\n        \n        # Create temp directory for audio outputs\n        temp_dir_audio = tempfile.mkdtemp(prefix='marvis_audio_demo_')\n        \n        # Evaluate on a small subset\n        audio_results = audio_classifier.predict_with_detailed_results(\n            audio_test_paths[:5],  # First 5 test samples\n            y_test_audio[:5],\n            save_visualizations=True,\n            output_dir=temp_dir_audio\n        )\n        \n        print(\"\\nüìà Audio Classification Results:\")\n        accuracy = audio_results.get('accuracy', 'N/A')\n        completed_samples = audio_results.get('completed_samples', 5)\n        \n        print(f\"   ‚Ä¢ Accuracy: {accuracy}\")\n        print(f\"   ‚Ä¢ Completed samples: {completed_samples}\")\n        \n        # Display predictions\n        predictions = audio_results.get('predictions', [])\n        if predictions:\n            print(\"\\nüéØ Sample Audio Predictions:\")\n            for i, pred in enumerate(predictions[:3]):\n                predicted_class = pred if isinstance(pred, str) else class_names_audio[pred] if pred < len(class_names_audio) else f\"Class {pred}\"\n                true_class = class_names_audio[y_test_audio[i]]\n                print(f\"   Sample {i+1}: Predicted='{predicted_class}', True='{true_class}'\")\n        \n        # Display audio visualization\n        print(\"\\nüñºÔ∏è Displaying t-SNE visualization for audio data...\")\n        \n        viz_files_audio = list(Path(temp_dir_audio).glob(\"*.png\"))\n        \n        if viz_files_audio:\n            latest_viz_audio = max(viz_files_audio, key=lambda p: p.stat().st_mtime)\n            print(f\"üìä Found audio visualization: {latest_viz_audio.name}\")\n            \n            display(IPImage(filename=str(latest_viz_audio), width=600))\n            \n            print(\"\\nüé® Audio Visualization Features:\")\n            print(\"   ‚Ä¢ Each point represents an audio sample in t-SNE space\")\n            print(\"   ‚Ä¢ Whisper embeddings capture audio frequency patterns\")\n            print(\"   ‚Ä¢ Different tones cluster based on frequency content\")\n            print(\"   ‚Ä¢ VLM recognizes patterns to classify audio by frequency\")\n        else:\n            print(\"‚ö†Ô∏è No audio visualization found.\")\n            \n    except Exception as e:\n        print(f\"‚ùå Error during audio training/evaluation: {e}\")\n        print(\"Audio classification requires Whisper and additional dependencies.\")\nelse:\n    print(\"‚ö†Ô∏è Skipping audio classification demo due to missing dependencies.\")\n    print(\"Audio classification would show:\")\n    print(\"   ‚Ä¢ Whisper embeddings of audio frequency patterns\")\n    print(\"   ‚Ä¢ t-SNE clustering of similar audio characteristics\")\n    print(\"   ‚Ä¢ VLM-based reasoning about audio frequency content\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Part 4: Multi-Visualization Reasoning\n",
    "\n",
    "MARVIS also supports **multi-visualization** mode, where multiple dimensionality reduction techniques are combined for richer visual reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate multi-visualization with tabular data\n",
    "print(\"üî¨ Demonstrating multi-visualization reasoning...\")\n",
    "\n",
    "# Initialize multi-viz classifier\n",
    "multi_viz_classifier = MarvisTsneClassifier(\n",
    "    modality=\"tabular\",\n",
    "    vlm_model_id=\"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
    "    enable_multi_viz=True,  # Enable multi-visualization\n",
    "    visualization_methods=['pca', 'tsne'],  # Use PCA and t-SNE\n",
    "    layout_strategy='adaptive_grid',\n",
    "    reasoning_focus='comparison',\n",
    "    tsne_perplexity=15,\n",
    "    tsne_max_iter=500,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Multi-visualization classifier initialized!\")\n",
    "print(\"   ‚Ä¢ Methods: PCA (linear) + t-SNE (non-linear)\")\n",
    "print(\"   ‚Ä¢ Layout: Adaptive grid for comparison\")\n",
    "print(\"   ‚Ä¢ Focus: Cross-method pattern comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Fit and evaluate multi-viz classifier\nprint(\"üèãÔ∏è Training multi-visualization classifier...\")\n\ntry:\n    multi_viz_classifier.fit(\n        X_train_tab,\n        y_train_tab,\n        X_test_tab[:15],  # Use subset\n        class_names=class_names_tab,\n        task_type='classification'  # Explicitly specify task type\n    )\n    \n    print(\"‚úÖ Multi-viz classifier trained!\")\n    \n    # Create temp directory for multi-viz outputs\n    temp_dir_multi = tempfile.mkdtemp(prefix='marvis_multi_viz_')\n    \n    # Evaluate with multi-visualization\n    print(\"\\nüîÆ Making multi-visualization predictions...\")\n    \n    multi_viz_results = multi_viz_classifier.predict_with_detailed_results(\n        X_test_tab[:8],\n        y_test_tab[:8],\n        save_visualizations=True,\n        output_dir=temp_dir_multi\n    )\n    \n    print(\"\\nüìà Multi-Visualization Results:\")\n    accuracy = multi_viz_results.get('accuracy', 'N/A')\n    completed_samples = multi_viz_results.get('completed_samples', 8)\n    \n    print(f\"   ‚Ä¢ Accuracy: {accuracy}\")\n    print(f\"   ‚Ä¢ Completed samples: {completed_samples}\")\n    \n    # Display multi-visualization\n    viz_files_multi = list(Path(temp_dir_multi).glob(\"*multi*.png\"))\n    \n    if viz_files_multi:\n        latest_viz_multi = max(viz_files_multi, key=lambda p: p.stat().st_mtime)\n        print(f\"\\nüìä Found multi-visualization: {latest_viz_multi.name}\")\n        \n        display(IPImage(filename=str(latest_viz_multi), width=800))\n        \n        print(\"\\nüé® Multi-Visualization Features:\")\n        print(\"   ‚Ä¢ Left panel: PCA projection (linear relationships)\")\n        print(\"   ‚Ä¢ Right panel: t-SNE projection (non-linear clusters)\")\n        print(\"   ‚Ä¢ VLM compares both views for robust classification\")\n        print(\"   ‚Ä¢ Cross-method consensus improves prediction confidence\")\n    else:\n        print(\"‚ö†Ô∏è No multi-visualization found, but classifier is functional.\")\n        \nexcept Exception as e:\n    print(f\"‚ùå Error in multi-visualization demo: {e}\")\n    print(\"Multi-visualization requires additional computational resources.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Part 5: Comparison and Summary\n",
    "\n",
    "Let's compare the performance across different modalities and summarize key insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of results across modalities\n",
    "print(\"üìä MARVIS Performance Summary Across Modalities\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_summary = {\n",
    "    \"Tabular Data\": {\n",
    "        \"samples\": len(X_test_tab),\n",
    "        \"features\": X_tabular.shape[1],\n",
    "        \"classes\": len(class_names_tab),\n",
    "        \"accuracy\": tabular_results.get('accuracy', 'N/A') if 'tabular_results' in locals() else 'N/A',\n",
    "        \"method\": \"Direct tabular embedding ‚Üí t-SNE\"\n",
    "    },\n",
    "    \"Image Data\": {\n",
    "        \"samples\": len(X_test_img),\n",
    "        \"features\": \"8x8 pixels (64 features)\",\n",
    "        \"classes\": len(class_names_img),\n",
    "        \"accuracy\": image_results.get('accuracy', 'N/A') if 'image_results' in locals() else 'N/A',\n",
    "        \"method\": \"Pixel features ‚Üí t-SNE\"\n",
    "    }\n",
    "}\n",
    "\n",
    "if audio_available and 'audio_results' in locals():\n",
    "    results_summary[\"Audio Data\"] = {\n",
    "        \"samples\": len(audio_test_paths),\n",
    "        \"features\": \"Whisper embeddings\",\n",
    "        \"classes\": len(class_names_audio),\n",
    "        \"accuracy\": audio_results.get('accuracy', 'N/A'),\n",
    "        \"method\": \"Audio ‚Üí Whisper ‚Üí t-SNE\"\n",
    "    }\n",
    "\n",
    "for modality, info in results_summary.items():\n",
    "    print(f\"\\nüéØ {modality}:\")\n",
    "    print(f\"   ‚Ä¢ Test samples: {info['samples']}\")\n",
    "    print(f\"   ‚Ä¢ Features: {info['features']}\")\n",
    "    print(f\"   ‚Ä¢ Classes: {info['classes']}\")\n",
    "    print(f\"   ‚Ä¢ Accuracy: {info['accuracy']}\")\n",
    "    print(f\"   ‚Ä¢ Method: {info['method']}\")\n",
    "\n",
    "print(\"\\nüéâ Key MARVIS Advantages:\")\n",
    "print(\"   ‚úÖ Unified interface across modalities (tabular, image, audio)\")\n",
    "print(\"   ‚úÖ Visual reasoning using state-of-the-art VLMs\")\n",
    "print(\"   ‚úÖ Interpretable predictions through visualization\")\n",
    "print(\"   ‚úÖ Multi-visualization support for robust classification\")\n",
    "print(\"   ‚úÖ Handles few-shot learning scenarios effectively\")\n",
    "print(\"   ‚úÖ No need for extensive hyperparameter tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization comparison\n",
    "print(\"üñºÔ∏è Visualization Techniques in MARVIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "viz_techniques = {\n",
    "    \"t-SNE\": \"Non-linear dimensionality reduction preserving local structure\",\n",
    "    \"PCA\": \"Linear dimensionality reduction showing global variance patterns\",\n",
    "    \"Multi-viz\": \"Combines multiple methods for comprehensive pattern analysis\",\n",
    "    \"KNN connections\": \"Shows nearest neighbor relationships for context\",\n",
    "    \"3D mode\": \"Three-dimensional projections for complex data structures\"\n",
    "}\n",
    "\n",
    "for technique, description in viz_techniques.items():\n",
    "    print(f\"\\nüîç {technique}:\")\n",
    "    print(f\"   {description}\")\n",
    "\n",
    "print(\"\\nüß† VLM Reasoning Process:\")\n",
    "reasoning_steps = [\n",
    "    \"1. Data is embedded into high-dimensional space\",\n",
    "    \"2. Dimensionality reduction creates 2D/3D visualization\", \n",
    "    \"3. Visualization is rendered as an image with color coding\",\n",
    "    \"4. VLM analyzes visual patterns, clusters, and relationships\",\n",
    "    \"5. VLM provides structured prediction with reasoning\",\n",
    "    \"6. Confidence scores based on pattern clarity and consistency\"\n",
    "]\n",
    "\n",
    "for step in reasoning_steps:\n",
    "    print(f\"   {step}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Next Steps and Advanced Usage\n",
    "\n",
    "Congratulations! You've successfully explored MARVIS's capabilities across multiple data modalities. Here are some advanced features and next steps to explore:\n",
    "\n",
    "### üîß Advanced Configuration Options\n",
    "\n",
    "```python\n",
    "# Advanced MARVIS configuration\n",
    "advanced_classifier = MarvisTsneClassifier(\n",
    "    modality=\"tabular\",\n",
    "    vlm_model_id=\"gpt-4o\",  # Use more powerful VLM\n",
    "    \n",
    "    # Multi-visualization settings\n",
    "    enable_multi_viz=True,\n",
    "    visualization_methods=['pca', 'tsne', 'umap', 'spectral'],\n",
    "    layout_strategy='hierarchical',\n",
    "    reasoning_focus='consensus',\n",
    "    \n",
    "    # 3D visualization\n",
    "    use_3d=True,\n",
    "    \n",
    "    # Enhanced KNN analysis\n",
    "    use_knn_connections=True,\n",
    "    nn_k=10,\n",
    "    \n",
    "    # High-quality rendering\n",
    "    max_vlm_image_size=1200,\n",
    "    image_dpi=150,\n",
    "    \n",
    "    # Semantic class names\n",
    "    use_semantic_names=True\n",
    ")\n",
    "```\n",
    "\n",
    "### üìö Useful Resources\n",
    "\n",
    "- **Examples directory**: Real-world datasets and evaluation scripts\n",
    "- **Scripts directory**: Utility tools for metadata, visualization, and results management\n",
    "- **Tests directory**: Comprehensive test suite and integration examples\n",
    "\n",
    "### üéØ Real-World Applications\n",
    "\n",
    "- **Tabular**: Financial data, medical records, survey responses\n",
    "- **Images**: Medical imaging, satellite imagery, document classification\n",
    "- **Audio**: Speech recognition, music classification, environmental sound analysis\n",
    "\n",
    "### üõ†Ô∏è Customization Options\n",
    "\n",
    "- **Custom embeddings**: Bring your own embedding models\n",
    "- **Domain-specific VLMs**: Use specialized vision-language models\n",
    "- **Visualization layouts**: Design custom multi-viz arrangements\n",
    "- **Prompt engineering**: Customize VLM reasoning prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup temporary directories\n",
    "print(\"üßπ Cleaning up temporary files...\")\n",
    "\n",
    "temp_dirs = []\n",
    "if 'temp_dir' in locals():\n",
    "    temp_dirs.append(temp_dir)\n",
    "if 'temp_dir_img' in locals():\n",
    "    temp_dirs.append(temp_dir_img)\n",
    "if 'audio_temp_dir' in locals():\n",
    "    temp_dirs.append(audio_temp_dir)\n",
    "if 'temp_dir_audio' in locals():\n",
    "    temp_dirs.append(temp_dir_audio)\n",
    "if 'temp_dir_multi' in locals():\n",
    "    temp_dirs.append(temp_dir_multi)\n",
    "\n",
    "for temp_dir_path in temp_dirs:\n",
    "    try:\n",
    "        shutil.rmtree(temp_dir_path)\n",
    "        print(f\"   ‚úÖ Cleaned: {temp_dir_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Could not clean {temp_dir_path}: {e}\")\n",
    "\n",
    "print(\"\\nüéâ MARVIS Getting Started tutorial completed!\")\n",
    "print(\"\\nüìñ Thank you for exploring MARVIS! Ready to classify your own data? üöÄ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}